{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################  READ REUTERS/bloomberg RAW NEWS INTO WORD2VEC INPUT FORMAT AND DO THE PREPROCCING\n",
    "##################### reuters_news.json/bloomberg_news.json --> reuters_bloomberg_train.json, Reuters.txt\n",
    "#replace(\"'s\", \"\"),(\"u.s.\", \"america\"),(\"american\", \"america\"),replace(\"update 1-\", \"\").replace(\"update 2-\", \"\").replace(\"update 3-\", \"\").replace(\"update 4-\", \"\")\n",
    "# \"wall st.\", \"wall street\"),(\"s&p 500\", \"standardpoor\"),(\"s&p\", \"standardpoor\"),(\"factbox:\", \"\").(\"analysis:\", \"\").(\"insight:\", \"\").(\"advisory:\", \"\")(\"bernanke:\", \"\").strip(\" \")\n",
    "import re\n",
    "import tqdm\n",
    "import collections\n",
    "import csv\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "import multiprocessing\n",
    "import threading\n",
    "import asyncio\n",
    "import time\n",
    "import functools\n",
    "import timeit\n",
    "import glob, os\n",
    "from os import listdir\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "#from nltk import pos_tag, word_tokenize\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.corpus import stopwords  # Import the stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pywsd's Lemmatizer.\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "tagger = PerceptronTagger()\n",
    "pos_tag = tagger.tag\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "def lemmatize(ambiguous_word, pos=None, neverstem=True,\n",
    "              lemmatizer=wnl, stemmer=porter):\n",
    "    \"\"\"\n",
    "    Tries to convert a surface word into lemma, and if lemmatize word is not in\n",
    "    wordnet then try and convert surface word into its stem.\n",
    "    This is to handle the case where users input a surface word as an ambiguous\n",
    "    word and the surface word is a not a lemma.\n",
    "    \"\"\"\n",
    "    if pos:\n",
    "        lemma = lemmatizer.lemmatize(ambiguous_word, pos=pos)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(ambiguous_word)\n",
    "    stem = stemmer.stem(ambiguous_word)\n",
    "    # Ensure that ambiguous word is a lemma.\n",
    "    if not wn.synsets(lemma):\n",
    "        if neverstem:\n",
    "            return ambiguous_word\n",
    "        if not wn.synsets(stem):\n",
    "            return ambiguous_word\n",
    "        else:\n",
    "            return stem\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "def penn2morphy(penntag, returnNone=False):\n",
    "    morphy_tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return None if returnNone else ''\n",
    "\n",
    "def word_tokenize(text,tokenize=tokenizer):\n",
    "    return tokenize.tokenize(text.lower())# doesn't remove stop words\n",
    "    #return [w for w in tokenize.tokenize(text.lower()) if not w in stopwords.words(\"english\")]\n",
    "    \n",
    "def lemmatize_sentence(sentence, neverstem=False, keepWordPOS=False,\n",
    "                       tokenizer=word_tokenize, postagger=pos_tag,\n",
    "                       lemmatizer=wnl, stemmer=porter):\n",
    "    words, lemmas, poss = [], [], []\n",
    "    for word, pos in postagger(tokenizer(sentence)):\n",
    "        pos = penn2morphy(pos)\n",
    "        lemmas.append(lemmatize(word.lower(), pos, neverstem,\n",
    "                                lemmatizer, stemmer))\n",
    "        poss.append(pos)\n",
    "        words.append(word)\n",
    "    if keepWordPOS:\n",
    "        return words, lemmas, [None if i == '' else i for i in poss]\n",
    "    sente = \"\"\n",
    "    for i in range(len(lemmas)):\n",
    "        sente = sente + lemmas[i] + \" \"\n",
    "    return sente\n",
    "\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    return re.sub(strip_special_chars, \" \", string.lower())\n",
    "\n",
    "\n",
    "def cleanparent(string):\n",
    "    string = re.sub(r\"\\(.*?\\)\", \" \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "def cleannumber(string):\n",
    "    nonum = \"\"\n",
    "    word_list = list(string.split())\n",
    "    for m in range(len(word_list)):\n",
    "        try:\n",
    "            word_list[m] = int(word_list[m])\n",
    "            word_list[m] = \"xxx\"\n",
    "            nonum = nonum + \" \" + word_list[m]\n",
    "        except ValueError:\n",
    "            nonum = nonum + \" \" + word_list[m]\n",
    "            continue\n",
    "    return nonum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"news_title/reuters_news.json\")\n",
    "last = \"\"\n",
    "output = open(\"news_title/Reuters.txt\", \"w\")\n",
    "training_file = open(\"news_title/reuters_bloomberg_train.json\", \"w\")\n",
    "bloom_file = open(\"news_title/bloomberg_news.json\")\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106494/106494 [00:08<00:00, 13046.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# READ REUTERS RAW NEWS INTO WORD2VEC INPUT FORMAT AND DO THE PREPROCCING\n",
    "for line in tqdm.tqdm(file.readlines()):\n",
    "    #no_num = \"\"\n",
    "    dic = json.loads(line)\n",
    "    raw_title = dic[\"title\"].lower()  # get raw title\n",
    "    raw_title = raw_title.replace(\"'s\", \"\").replace(\"u.s.\", \"america\").replace(\"american\", \"america\")\n",
    "    raw_title = raw_title.replace(\"update 1-\", \"\").replace(\"update 2-\", \"\").replace(\"update 3-\", \"\").replace(\"update 4-\", \"\")\\\n",
    "        .replace(\"update 5-\", \"\").replace(\"update 6-\", \"\").replace(\"update 7-\", \"\").replace(\"update 8-\", \"\")\\\n",
    "        .replace(\"update 9-\", \"\").replace(\"update 10-\", \"\")\n",
    "    raw_title = raw_title.replace(\"wall st.\", \"wall street\").replace(\"s&p 500\", \"standardpoor\").replace(\"s&p\", \"standardpoor\")\n",
    "    raw_title = raw_title.replace(\"factbox:\", \"\").replace(\"analysis:\", \"\").replace(\"insight:\", \"\").replace(\"advisory:\", \"\")\\\n",
    "        .replace(\"bernanke:\", \"\").strip(\" \")\n",
    "    no_pun_title = cleanSentences(raw_title)\n",
    "    no_num_title = cleannumber(no_pun_title)\n",
    "    if no_num_title == last:  # eliminate same news\n",
    "        continue\n",
    "        \n",
    "    #  finish deal with title\n",
    "    raw_abstract = dic[\"abstract\"].lower()\n",
    "    raw_abstract = cleanparent(raw_abstract)\n",
    "    raw_abstract = raw_abstract.replace(\"'s\", \"\").replace(\"u.s.\", \"america\").replace(\"american\", \"america\")\n",
    "    raw_abstract = raw_abstract.replace(\"update 1-\", \"\").replace(\"update 2-\", \"\").replace(\"update 3-\", \"\").replace(\"update 4-\", \"\") \\\n",
    "        .replace(\"update 5-\", \"\").replace(\"update 6-\", \"\").replace(\"update 7-\", \"\").replace(\"update 8-\", \"\") \\\n",
    "        .replace(\"update 9-\", \"\").replace(\"update 10-\", \"\")\n",
    "    raw_abstract = raw_abstract.replace(\"wall st.\", \"wall street\").replace(\"s&p 500\", \"standardpoor\").replace(\"s&p\", \"standardpoor\")\n",
    "    raw_abstract = raw_abstract.replace(\"factbox:\", \"\").replace(\"analysis:\", \"\").replace(\"insight:\", \"\").replace(\"advisory:\", \"\") \\\n",
    "        .replace(\"bernanke:\", \"\").strip(\" \")\n",
    "    no_pun_abstract = cleanSentences(raw_abstract)\n",
    "    no_num_abstract = cleannumber(no_pun_abstract)\n",
    "\n",
    "    #  finish deal with abstract\n",
    "    raw_article = dic[\"article\"].lower()\n",
    "    raw_article = cleanparent(raw_article)\n",
    "    raw_article = raw_article.replace(\"'s\", \"\").replace(\"u.s.\", \"america\").replace(\"american\", \"america\")\n",
    "    raw_article = raw_article.replace(\"update 1-\", \"\").replace(\"update 2-\", \"\").replace(\"update 3-\", \"\").replace(\"update 4-\",\n",
    "                                                                                                          \"\") \\\n",
    "        .replace(\"update 5-\", \"\").replace(\"update 6-\", \"\").replace(\"update 7-\", \"\").replace(\"update 8-\", \"\") \\\n",
    "        .replace(\"update 9-\", \"\").replace(\"update 10-\", \"\")\n",
    "    raw_article = raw_article.replace(\"wall st.\", \"wall street\").replace(\"s&p 500\", \"standardpoor\").replace(\"s&p\", \"standardpoor\")\n",
    "    raw_article = raw_article.replace(\"factbox:\", \"\").replace(\"analysis:\", \"\").replace(\"insight:\", \"\").replace(\"advisory:\", \"\") \\\n",
    "        .replace(\"bernanke:\", \"\").strip(\" \")\n",
    "    no_pun_article = cleanSentences(raw_article)\n",
    "    no_num_article = cleannumber(no_pun_article)\n",
    "    #  finish deal with article\n",
    "    \n",
    "    print(no_num_title.strip(), end=\" \", file=output)\n",
    "    #print(no_num_abstract.strip(), end=\" \", file=output)\n",
    "    #print(no_num_article.strip(), end=\" \", file=output)\n",
    "    print(dic[\"date\"], \"\\t\", no_num_title.strip(\" \"), file=output)\n",
    "    data = {\"date\": \"\", \"title\": \"\", \"abstract\": \"\", \"article\": \"\"}\n",
    "    data[\"date\"] = dic[\"date\"]\n",
    "    data[\"title\"] = no_num_title.strip()\n",
    "    data[\"abstract\"] = no_num_abstract.strip()\n",
    "    data[\"article\"] = no_num_article.strip()\n",
    "    o_data = collections.OrderedDict(data)\n",
    "    json_o = json.dump(o_data, training_file)\n",
    "    print(\"\\r\", file=training_file)\n",
    "    last = no_num_title\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447376/447376 [00:42<00:00, 10533.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553748\n"
     ]
    }
   ],
   "source": [
    "# READ BLOOMBERG RAW NEWS INTO WORD2VEC INPUT FORMAT AND DO THE PREPROCCING\n",
    "for line in tqdm.tqdm(bloom_file.readlines()):\n",
    "    # no_num = \"\"\n",
    "    dic = json.loads(line)\n",
    "    raw_title = dic[\"title\"].lower()  # get raw title\n",
    "    raw_title = raw_title.replace(\"'s\", \"\").replace(\"u.s.\", \"america\").replace(\"american\", \"america\")\n",
    "    raw_title = raw_title.replace(\"update 1-\", \"\").replace(\"update 2-\", \"\").replace(\"update 3-\", \"\").replace(\n",
    "        \"update 4-\", \"\") \\\n",
    "        .replace(\"update 5-\", \"\").replace(\"update 6-\", \"\").replace(\"update 7-\", \"\").replace(\"update 8-\", \"\") \\\n",
    "        .replace(\"update 9-\", \"\").replace(\"update 10-\", \"\")\n",
    "    raw_title = raw_title.replace(\"wall st.\", \"wall street\").replace(\"s&p 500\", \"standardpoor\").replace(\"s&p\",\n",
    "                                                                                                        \"standardpoor\")\n",
    "    raw_title = raw_title.replace(\"factbox:\", \"\").replace(\"analysis:\", \"\").replace(\"insight:\", \"\").replace(\"advisory:\",\n",
    "                                                                                                           \"\") \\\n",
    "        .replace(\"bernanke:\", \"\").strip(\" \")\n",
    "    no_pun_title = cleanSentences(raw_title)\n",
    "    no_num_title = cleannumber(no_pun_title)\n",
    "    if no_num_title == last:  # eliminate same news\n",
    "        continue\n",
    "\n",
    "    print(no_num_title.strip(), end=\" \", file=output)\n",
    "    #print(no_num_abstract.strip(), end=\" \", file=output)\n",
    "    #print(no_num_article.strip(), end=\" \", file=output)\n",
    "    print(dic[\"date\"], \"\\t\", no_num_title.strip(\" \"), file=output)\n",
    "    data = {\"date\": \"\", \"title\": \"\"}\n",
    "    data[\"date\"] = dic[\"date\"]\n",
    "    data[\"title\"] = no_num_title.strip()\n",
    "    o_data = collections.OrderedDict(data)\n",
    "    json_o = json.dump(o_data, training_file)\n",
    "    print(\"\\r\", file=training_file)\n",
    "    last = no_num_title\n",
    "    counter = counter + 1\n",
    "\n",
    "file.close()\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
