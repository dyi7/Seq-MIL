{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import timeit\n",
    "import glob, os\n",
    "from os import listdir\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from gensim.utils import SaveLoad\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from itertools import islice\n",
    "from gensim.models import KeyedVectors\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "#from nltk import pos_tag, word_tokenize\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.corpus import stopwords  # Import the stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pywsd's Lemmatizer.\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "tagger = PerceptronTagger()\n",
    "pos_tag = tagger.tag\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "def lemmatize(ambiguous_word, pos=None, neverstem=True, \n",
    "              lemmatizer=wnl, stemmer=porter):\n",
    "    \"\"\"\n",
    "    Tries to convert a surface word into lemma, and if lemmatize word is not in\n",
    "    wordnet then try and convert surface word into its stem.\n",
    "    This is to handle the case where users input a surface word as an ambiguous \n",
    "    word and the surface word is a not a lemma.\n",
    "    \"\"\"\n",
    "    if pos:\n",
    "        lemma = lemmatizer.lemmatize(ambiguous_word, pos=pos)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(ambiguous_word)\n",
    "    stem = stemmer.stem(ambiguous_word)\n",
    "    # Ensure that ambiguous word is a lemma.\n",
    "    if not wn.synsets(lemma):\n",
    "        if neverstem:\n",
    "            return ambiguous_word\n",
    "        if not wn.synsets(stem):\n",
    "            return ambiguous_word\n",
    "        else:\n",
    "            return stem\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "def penn2morphy(penntag, returnNone=False):\n",
    "    morphy_tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return None if returnNone else ''\n",
    "\n",
    "def word_tokenize(text,tokenize=tokenizer):\n",
    "    #return tokenize.tokenize(text.lower())# doesn't remove stop words\n",
    "    return [w for w in tokenize.tokenize(text.lower()) if not w in stopwords.words(\"english\")]\n",
    "\n",
    "def lemmatize_sentence(sentence, neverstem=False, keepWordPOS=False, \n",
    "                       tokenizer=word_tokenize, postagger=pos_tag, \n",
    "                       lemmatizer=wnl, stemmer=porter):\n",
    "    words, lemmas, poss = [], [], []\n",
    "    for word, pos in postagger(tokenizer(sentence)):\n",
    "        pos = penn2morphy(pos)\n",
    "        lemmas.append(lemmatize(word.lower(), pos, neverstem,\n",
    "                                lemmatizer, stemmer))\n",
    "        poss.append(pos)\n",
    "        words.append(word)\n",
    "    if keepWordPOS:\n",
    "        return words, lemmas, [None if i == '' else i for i in poss]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path, sentence_stream):\n",
    "    count = 0\n",
    "    subfolder_list = glob.glob(path +'/*')\n",
    "    pbar = tqdm(total=len(subfolder_list))\n",
    "    folders = [x[0] for x in os.walk(path)]\n",
    "    for x in folders[1:-1]:\n",
    "        #print(count)\n",
    "        pbar.set_description('Extracting {}'.format(x))\n",
    "        pbar.update(1)\n",
    "        for i in listdir(x):\n",
    "            count += 1\n",
    "            with open(x + \"/\" + i, 'r') as f:\n",
    "                lines = (line for line in islice(f, 7, None))\n",
    "                try:\n",
    "                    for line in lines:\n",
    "                        line = re.sub(\"\\d+\\S\\d+\", 'xxx', line)#remove numbers\n",
    "                        line = re.sub(\"\\d+\", 'xxx', line)#remove numbers\n",
    "                        sentence_stream +=[lemmatize_sentence(i) for i in line.strip().split('. ')]\n",
    "                        #sentence_stream +=[word_tokenize(i) for i in line.strip().split('. ')]\n",
    "                        '''sentence_stream += [\n",
    "                            list(filter(None, i.strip().lower().translate(\n",
    "                                str.maketrans(string.punctuation, ' ' * len(string.punctuation))).split(' '))) for i\n",
    "                            in line.strip().split('. ')]'''\n",
    "                        # line=[i.strip().lower().replace('^[{}]'.format(string.punctuation), ' ') for i in\n",
    "                        #                   line]\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(str(e))\n",
    "    pbar.close()\n",
    "        # get rid of the null in list.\n",
    "        # replace punctuation with ' '\n",
    "        # stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2588 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "start the reuters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ReutersNews106521/20131120:  79%|███████▉  | 2045/2588 [10:31:51<1:35:45, 10.58s/it]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ReutersNews106521/20110303: 100%|██████████| 2588/2588 [13:13:51<00:00, 18.40s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404385\n",
      "after:2330421\n",
      "0.19168934412300587\n"
     ]
    }
   ],
   "source": [
    "sentence_stream = []\n",
    "#sentence_stream1 = []\n",
    "#sentence_stream_our = []\n",
    "start = timeit.default_timer()\n",
    "print(\"=\"*80)\n",
    "print('start the reuters')\n",
    "extract(\"ReutersNews106521\", sentence_stream)\n",
    "print(len(sentence_stream))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "sentence_stream = list(filter(None, sentence_stream))\n",
    "print('after:{}'.format(len(sentence_stream)))\n",
    "print(timeit.default_timer() - start)\n",
    "#with open(\"sentence.csv\", \"w\") as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    writer.writerows(sentence_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bi-gram/ tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phrases = Phrases(sentence_stream,min_count=500, threshold=2)\\nbigram = Phraser(phrases)\\n# print(list(bigram[sentence_stream]))\\nprint(bigram[\\'u\\', \\'s\\', \\'wall\\', \\'st\\', \\'wall\\', \\'street\\',\\'s\\',\\'p\\',\\'500\\',\\'s\\',\\'p\\',\\'xxx\\'])\\n##TODO phrase\\n#bigram.save(\"big_phrase.pickle\")\\nprint(\\'finish phrase time:{}\\'.format(timeit.default_timer() - start))\\n\\nprint(\\'start trigram\\')\\nstart = timeit.default_timer()\\nphrases = Phrases(bigram[sentence_stream],min_count=500, threshold=2)\\ntrigram = Phraser(phrases)\\n#trigram.save(\"trig_phrase.pickle\")\\nprint(trigram[bigram[\\'u\\', \\'s\\', \\'wall\\', \\'st\\', \\'wall\\', \\'street\\',\\'bank\\',\\'of\\',\\'america\\',\\'s\\',\\'p\\',\\'500\\',\\'s\\',\\'p\\',\\'xxx\\']])\\nprint(\\'finish phrase time:{}\\'.format(timeit.default_timer() - start))'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"phrases = Phrases(sentence_stream,min_count=500, threshold=2)\n",
    "bigram = Phraser(phrases)\n",
    "# print(list(bigram[sentence_stream]))\n",
    "print(bigram['u', 's', 'wall', 'st', 'wall', 'street','s','p','500','s','p','xxx'])\n",
    "##TODO phrase\n",
    "#bigram.save(\"big_phrase.pickle\")\n",
    "print('finish phrase time:{}'.format(timeit.default_timer() - start))\n",
    "\n",
    "print('start trigram')\n",
    "start = timeit.default_timer()\n",
    "phrases = Phrases(bigram[sentence_stream],min_count=500, threshold=2)\n",
    "trigram = Phraser(phrases)\n",
    "#trigram.save(\"trig_phrase.pickle\")\n",
    "print(trigram[bigram['u', 's', 'wall', 'st', 'wall', 'street','bank','of','america','s','p','500','s','p','xxx']])\n",
    "print('finish phrase time:{}'.format(timeit.default_timer() - start))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading sentence from file\n"
     ]
    }
   ],
   "source": [
    "#import cpickle as pickle\n",
    "print(\"reading sentence from file\")\n",
    "with open(\"sentence.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    sentence_stream=list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(sentence_stream, min_count=500, threshold=2)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "#print(bigram_phraser)\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    tokens_ = bigram_phraser[sent]\n",
    "    #print(tokens_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 's', 'wall', 'st', 'wall_street', 's', 'p', '500', 's', 'p', 'xxx']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_phraser['u', 's', 'wall', 'st', 'wall', 'street','s','p','500','s','p','xxx'])\n",
    "#bigram_phraser.save(\"big_phrase.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 's', 'wall', 'st', 'wall_street', 'bank', 'of', 'america', 's', 'p', '500', 's', 'p_xxx']\n"
     ]
    }
   ],
   "source": [
    "trigram = Phrases(bigram[sentence_stream],min_count=500, threshold=2)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "#trigram.save(\"trig_phrase.pickle\")\n",
    "print(trigram_phraser[bigram_phraser['u', 's', 'wall', 'st', 'wall', 'street','bank','of','america','s','p','500','s','p','xxx']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigram_phraser.save(\"trig_phrase.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Build the bigram and trigram models\n",
    "bigram1 = gensim.models.Phrases(sentence_stream, min_count=5, threshold=100) \n",
    "trigram1 = gensim.models.Phrases(bigram1[sentence_stream], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram1)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['reuters',\n",
       "  'japan',\n",
       "  'disgraced',\n",
       "  'olympus',\n",
       "  'corp',\n",
       "  'xxx',\n",
       "  'may',\n",
       "  'sell',\n",
       "  'asset',\n",
       "  'help',\n",
       "  'pay',\n",
       "  'xxx',\n",
       "  'billion',\n",
       "  'debt',\n",
       "  'plan',\n",
       "  'aim',\n",
       "  'keep',\n",
       "  'support',\n",
       "  'lender',\n",
       "  'battle',\n",
       "  'survive',\n",
       "  'account',\n",
       "  'scandal',\n",
       "  'nikkei',\n",
       "  'business',\n",
       "  'daily',\n",
       "  'say',\n",
       "  'thursday'],\n",
       " ['olympus',\n",
       "  'bank',\n",
       "  'creditor',\n",
       "  'crucial',\n",
       "  'prospect',\n",
       "  'come',\n",
       "  'scandal',\n",
       "  'give',\n",
       "  'company',\n",
       "  'relatively',\n",
       "  'highly',\n",
       "  'gear',\n",
       "  'expect',\n",
       "  'make',\n",
       "  'hefty',\n",
       "  'writedowns',\n",
       "  'account',\n",
       "  'put',\n",
       "  'straight'],\n",
       " ['proud',\n",
       "  'maker',\n",
       "  'cameras_medical',\n",
       "  'equipment',\n",
       "  'put',\n",
       "  'forward',\n",
       "  'proposal',\n",
       "  'meeting',\n",
       "  'creditor',\n",
       "  'wednesday',\n",
       "  'offer',\n",
       "  'cut',\n",
       "  'debt',\n",
       "  'xxx',\n",
       "  'billion',\n",
       "  'yen',\n",
       "  'xxx',\n",
       "  'billion',\n",
       "  'next',\n",
       "  'three',\n",
       "  'year',\n",
       "  'nikkei',\n",
       "  'say'],\n",
       " ['appear',\n",
       "  'consider',\n",
       "  'sell',\n",
       "  'asset',\n",
       "  'mean',\n",
       "  'repay',\n",
       "  'debt',\n",
       "  'addition',\n",
       "  'tap',\n",
       "  'cash',\n",
       "  'reserve',\n",
       "  'cash',\n",
       "  'flow',\n",
       "  'newspaper',\n",
       "  'say',\n",
       "  'unsourced',\n",
       "  'report',\n",
       "  'although',\n",
       "  'quote',\n",
       "  'senior',\n",
       "  'banker',\n",
       "  'say',\n",
       "  'olympus',\n",
       "  'face',\n",
       "  'imminent',\n",
       "  'cash',\n",
       "  'crunch'],\n",
       " ['company',\n",
       "  'lose',\n",
       "  'xxx',\n",
       "  'percent',\n",
       "  'market',\n",
       "  'value',\n",
       "  'investigation',\n",
       "  'police',\n",
       "  'regulator',\n",
       "  'admit',\n",
       "  'month',\n",
       "  'hide',\n",
       "  'investment',\n",
       "  'loss',\n",
       "  'investor',\n",
       "  'decade',\n",
       "  'use',\n",
       "  'payment',\n",
       "  'link',\n",
       "  'merger',\n",
       "  'acquisition',\n",
       "  'aid',\n",
       "  'cover'],\n",
       " ['payment',\n",
       "  'include',\n",
       "  'huge',\n",
       "  'xxx',\n",
       "  'million',\n",
       "  'fee',\n",
       "  'pay',\n",
       "  'obscure',\n",
       "  'financial',\n",
       "  'adviser',\n",
       "  'olympus',\n",
       "  'xxx',\n",
       "  'billion',\n",
       "  'purchase',\n",
       "  'uk',\n",
       "  'medical',\n",
       "  'equipment',\n",
       "  'firm',\n",
       "  'gyrus',\n",
       "  'xxx'],\n",
       " ['fee', 'world', 'big', 'accord', 'thomson', 'reuters', 'data'],\n",
       " ['olympus',\n",
       "  'tell',\n",
       "  'creditor',\n",
       "  'acquisition',\n",
       "  'cost',\n",
       "  'gyrus',\n",
       "  'overstate',\n",
       "  'xxx',\n",
       "  'billion',\n",
       "  'yen',\n",
       "  'xxx',\n",
       "  'million',\n",
       "  'end',\n",
       "  'fiscal',\n",
       "  'xxx',\n",
       "  'nikkei',\n",
       "  'say',\n",
       "  'though',\n",
       "  'independent',\n",
       "  'panel',\n",
       "  'commission',\n",
       "  'olympus',\n",
       "  'still',\n",
       "  'probe',\n",
       "  'matter'],\n",
       " ['asahi_shimbun',\n",
       "  'newspaper',\n",
       "  'say',\n",
       "  'olympus',\n",
       "  'would',\n",
       "  'write',\n",
       "  'amount',\n",
       "  'book',\n",
       "  'though',\n",
       "  'added',\n",
       "  'equity',\n",
       "  'would',\n",
       "  'still',\n",
       "  'exceed',\n",
       "  'net',\n",
       "  'debt',\n",
       "  'restatement'],\n",
       " ['wednesday',\n",
       "  'meeting',\n",
       "  'involve',\n",
       "  'xxx',\n",
       "  'banker',\n",
       "  'two',\n",
       "  'major',\n",
       "  'creditor',\n",
       "  'sumitomo_mitsui',\n",
       "  'banking',\n",
       "  'corp',\n",
       "  'bank',\n",
       "  'tokyo',\n",
       "  'mitsubishi_ufj',\n",
       "  'btmu',\n",
       "  'say',\n",
       "  'would',\n",
       "  'continue',\n",
       "  'support',\n",
       "  'firm',\n",
       "  'multiple',\n",
       "  'source',\n",
       "  'tell',\n",
       "  'reuters']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigram_mod[sentence_stream])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 's', 'wall', 'st', 'wall_street', 's', 'p', '500', 's', 'p', 'xxx']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_mod['u', 's', 'wall', 'st', 'wall', 'street','s','p','500','s','p','xxx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
