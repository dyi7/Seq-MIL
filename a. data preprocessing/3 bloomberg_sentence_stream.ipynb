{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import glob, os\n",
    "from os import listdir\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from gensim.utils import SaveLoad\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from itertools import islice\n",
    "from gensim.models import KeyedVectors\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "#from nltk import pos_tag, word_tokenize\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.corpus import stopwords  # Import the stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pywsd's Lemmatizer.\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "tagger = PerceptronTagger()\n",
    "pos_tag = tagger.tag\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "def lemmatize(ambiguous_word, pos=None, neverstem=True, \n",
    "              lemmatizer=wnl, stemmer=porter):\n",
    "    \"\"\"\n",
    "    Tries to convert a surface word into lemma, and if lemmatize word is not in\n",
    "    wordnet then try and convert surface word into its stem.\n",
    "    This is to handle the case where users input a surface word as an ambiguous \n",
    "    word and the surface word is a not a lemma.\n",
    "    \"\"\"\n",
    "    if pos:\n",
    "        lemma = lemmatizer.lemmatize(ambiguous_word, pos=pos)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(ambiguous_word)\n",
    "    stem = stemmer.stem(ambiguous_word)\n",
    "    # Ensure that ambiguous word is a lemma.\n",
    "    if not wn.synsets(lemma):\n",
    "        if neverstem:\n",
    "            return ambiguous_word\n",
    "        if not wn.synsets(stem):\n",
    "            return ambiguous_word\n",
    "        else:\n",
    "            return stem\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "def penn2morphy(penntag, returnNone=False):\n",
    "    morphy_tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return None if returnNone else ''\n",
    "\n",
    "def word_tokenize(text,tokenize=tokenizer):\n",
    "    #return tokenize.tokenize(text.lower())# doesn't remove stop words\n",
    "    return [w for w in tokenize.tokenize(text.lower()) if not w in stopwords.words(\"english\")]\n",
    "\n",
    "def lemmatize_sentence(sentence, neverstem=False, keepWordPOS=False, \n",
    "                       tokenizer=word_tokenize, postagger=pos_tag, \n",
    "                       lemmatizer=wnl, stemmer=porter):\n",
    "    words, lemmas, poss = [], [], []\n",
    "    for word, pos in postagger(tokenizer(sentence)):\n",
    "        pos = penn2morphy(pos)\n",
    "        lemmas.append(lemmatize(word.lower(), pos, neverstem,\n",
    "                                lemmatizer, stemmer))\n",
    "        poss.append(pos)\n",
    "        words.append(word)\n",
    "    if keepWordPOS:\n",
    "        return words, lemmas, [None if i == '' else i for i in poss]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path, sentence_stream):\n",
    "    count = 0\n",
    "    subfolder_list = glob.glob(path +'/*')\n",
    "    pbar = tqdm(total=len(subfolder_list))\n",
    "    folders = [x[0] for x in os.walk(path)]\n",
    "    for x in folders[1:-1]:\n",
    "        #print(count)\n",
    "        pbar.set_description('Extracting {}'.format(x))\n",
    "        pbar.update(1)\n",
    "        for i in listdir(x):\n",
    "            count += 1\n",
    "            with open(x + \"/\" + i, 'r') as f:\n",
    "                lines = (line for line in islice(f, 7, None))\n",
    "                try:\n",
    "                    for line in lines:\n",
    "                        line = re.sub(\"\\d+\\S\\d+\", 'xxx', line)#remove numbers\n",
    "                        line = re.sub(\"\\d+\", 'xxx', line)#remove numbers\n",
    "                        sentence_stream +=[lemmatize_sentence(i) for i in line.strip().split('. ')]\n",
    "                        #sentence_stream +=[word_tokenize(i) for i in line.strip().split('. ')]\n",
    "                        '''sentence_stream += [\n",
    "                            list(filter(None, i.strip().lower().translate(\n",
    "                                str.maketrans(string.punctuation, ' ' * len(string.punctuation))).split(' '))) for i\n",
    "                            in line.strip().split('. ')]'''\n",
    "                        # line=[i.strip().lower().replace('^[{}]'.format(string.punctuation), ' ') for i in\n",
    "                        #                   line]\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(str(e))\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1944 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "start the bloombergs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting 20061020_20131126_bloomberg_news/2011-06-13: 100%|█████████▉| 1943/1944 [69:18:55<02:08, 128.43s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23970107\n",
      "after:23284555\n",
      "0.9336905162781477\n"
     ]
    }
   ],
   "source": [
    "sentence_stream_bloom = []\n",
    "#sentence_stream_our = []\n",
    "start = timeit.default_timer()\n",
    "print(\"=\"*80)\n",
    "print('start the bloombergs')\n",
    "extract(\"20061020_20131126_bloomberg_news\", sentence_stream_bloom)\n",
    "print(len(sentence_stream_bloom))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "sentence_stream = list(filter(None, sentence_stream_bloom))\n",
    "print('after:{}'.format(len(sentence_stream)))\n",
    "print(timeit.default_timer() - start)\n",
    "#with open(\"sentence_bloom.csv\", \"w\") as f:\n",
    " #   writer = csv.writer(f)\n",
    "  #  writer.writerows(sentence_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sentence_reuters.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    sentence_stream=list(reader)\n",
    "\n",
    "    \n",
    "with open(\"sentence_bloom.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    sentence_stream_bloom=list(reader)\n",
    "    \n",
    "sentence_stream  += sentence_stream_bloom\n",
    "#print(sentence_stream.shape)\n",
    "with open(\"sentence.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sentence_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 's', 'wall', 'st', 'wall_street', 's', 'p', '500', 's', 'p', 'xxx']\n"
     ]
    }
   ],
   "source": [
    "with open(\"sentence.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    sentence_stream=list(reader)\n",
    "\n",
    "phrases = Phrases(sentence_stream,min_count=500, threshold=2)\n",
    "bigram = Phraser(phrases)\n",
    "# print(list(bigram[sentence_stream]))\n",
    "print(bigram['u', 's', 'wall', 'st', 'wall', 'street','s','p','500','s','p','xxx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram.save(\"big_phrase.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start trigram\n",
      "['u', 's', 'wall', 'st', 'wall_street', 'bank', 'of', 'america', 's', 'p', '500', 's', 'p', 'xxx']\n",
      "finish phrase time:841.0161524452269\n"
     ]
    }
   ],
   "source": [
    "print('start trigram')\n",
    "start = timeit.default_timer()\n",
    "phrases = Phrases(bigram[sentence_stream],min_count=500, threshold=2)\n",
    "trigram = Phraser(phrases)\n",
    "trigram.save(\"trig_phrase.pickle\")\n",
    "print(trigram[bigram['u', 's', 'wall', 'st', 'wall', 'street','bank','of','america','s','p','500','s','p','xxx']])\n",
    "print('finish phrase time:{}'.format(timeit.default_timer() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
