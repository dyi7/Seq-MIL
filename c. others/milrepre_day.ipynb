{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sequence, sequence_d1, sequence_d2, labels, options, maxlen=None, max_word=100):\n",
    "    # length = [len(s) for s in sequence]\n",
    "    length, length_d1, length_d2 = [], [], [] #一天里面 number of news\n",
    "    for i, d1, d2 in zip(sequence, sequence_d1, sequence_d2): #x,one batch里面one day\n",
    "        dd1, dd2 = list(), list()\n",
    "        length.append(len(i))\n",
    "        for day in d1:\n",
    "            dd1.append(len(day))\n",
    "        length_d1.append(dd1)\n",
    "        for day in d2:\n",
    "            dd2.append(len(day))\n",
    "        length_d2.append(dd2)\n",
    "    if maxlen is not None:  # max length is the news level\n",
    "        new_sequence = []\n",
    "        new_lengths = []\n",
    "        new_sequence_d1 = []\n",
    "        new_lengths_d1 = []\n",
    "        new_sequence_d2 = []\n",
    "        new_lengths_d2 = []\n",
    "        for l, s, ld1, sd1, ld2, sd2 in zip(length, sequence, length_d1, sequence_d1, length_d2, sequence_d2):\n",
    "            dd1, lld1, dd2, lld2 = list(), list(), list(), list()\n",
    "            if l < maxlen:\n",
    "                new_sequence.append(s)\n",
    "                new_lengths.append(l)\n",
    "            for i, j in zip(ld1, sd1):\n",
    "                if i < maxlen:\n",
    "                    dd1.append(j)\n",
    "                    lld1.append(i)\n",
    "            new_sequence_d1.append(dd1)\n",
    "            new_lengths_d1.append(lld1)\n",
    "            for i, j in zip(ld2, sd2):\n",
    "                if i < maxlen:\n",
    "                    dd2.append(j)\n",
    "                    lld2.append(i)\n",
    "            new_sequence_d2.append(dd2)\n",
    "            new_lengths_d2.append(lld2)\n",
    "\n",
    "        length = new_lengths  # This step is to filter the sentence which length is bigger\n",
    "        sequence = new_sequence  # than the max length. length means number of news. sequence means \n",
    "        # length of each sentence\n",
    "        length_d1 = new_lengths_d1\n",
    "        sequence_d1 = new_sequence_d1\n",
    "        length_d2 = new_lengths_d2\n",
    "        sequence_d2 = new_sequence_d2\n",
    "        ##TODO need to be careful, set the max length bigger to avoid bug\n",
    "        if len(length) < 1:\n",
    "            return None, None, None, None, None, None, None, None\n",
    "    \n",
    "    day1 = options['delay1']-1\n",
    "    day2 = options['delay2']-options['delay1']\n",
    "    maxlen_x = numpy.max(length)  # max time step\n",
    "    try:\n",
    "        maxlen_xd1 = numpy.max([numpy.max(i) for i in length_d1])\n",
    "        maxlen_xd2 = numpy.max([numpy.max(i) for i in length_d2])\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "        maxlen_xd1=100\n",
    "        maxlen_xd2=100\n",
    "    n_samples = len(sequence)  # number of samples== batch\n",
    "    max_sequence = max(len(j) for i in sequence for j in i)  # find the sequence max length\n",
    "    max_sequence_d1 = max(len(j) for i in sequence_d1 for z in i for j in z)\n",
    "    max_sequence_d2 = max(len(j) for i in sequence_d2 for z in i for j in z)\n",
    "    max_sequence = max_word if max_sequence > max_word else max_sequence  # shrink the data size\n",
    "    max_sequence_d1 = max_word if max_sequence_d1 > max_word else max_sequence_d1  # shrink the data size\n",
    "    max_sequence_d2 = max_word if max_sequence_d2 > max_word else max_sequence_d2  # shrink the data size\n",
    "    ##TODO for x\n",
    "    x = numpy.zeros((n_samples, maxlen_x, max_sequence)).astype('int64')\n",
    "    x_mask = numpy.zeros((n_samples, maxlen_x)).astype('float32')\n",
    "    ##TODO for x_d1\n",
    "    x_d1 = numpy.zeros((n_samples, day1, maxlen_xd1, max_sequence_d1)).astype('int64')\n",
    "    x_d1_mask = numpy.zeros((n_samples, day1, maxlen_xd1)).astype('float32')\n",
    "    ##TODO for x_d2\n",
    "    x_d2 = numpy.zeros((n_samples, day2, maxlen_xd2, max_sequence_d2)).astype('int64')\n",
    "    x_d2_mask = numpy.zeros((n_samples, day2, maxlen_xd2)).astype('float32')\n",
    "    final_mask = numpy.ones((n_samples, 1 + day1 + day2)).astype('float32')\n",
    "    # l = numpy.array(labels).astype('int64')\n",
    "    ##TODO for label\n",
    "    l = numpy.zeros((n_samples,)).astype('int64')\n",
    "    for index, (i, j, k, ll) in enumerate(zip(sequence, sequence_d1, sequence_d2, labels)):  # batch size\n",
    "        l[index] = ll\n",
    "        for idx, ss in enumerate(i):  # time step\n",
    "            if len(ss) < max_sequence:\n",
    "                x[index, idx, :len(ss)] = ss\n",
    "            else:\n",
    "                x[index, idx, :max_sequence] = ss[:max_sequence]\n",
    "            x_mask[index, idx] = 1.\n",
    "        for jj, day in enumerate(j):\n",
    "            for idx, ss in enumerate(day):\n",
    "                if len(ss) < max_sequence_d1:\n",
    "                    x_d1[index, jj, idx, :len(ss)] = ss\n",
    "                else:\n",
    "                    x_d1[index, jj, idx, :max_sequence_d1] = ss[:max_sequence_d1]\n",
    "                x_d1_mask[index, jj, idx] = 1.\n",
    "        for kk, day in enumerate(k):\n",
    "            for idx, ss in enumerate(day):\n",
    "                if len(ss) < max_sequence_d2:\n",
    "                    x_d2[index, kk, idx, :len(ss)] = ss\n",
    "                else:\n",
    "                    x_d2[index, kk, idx, :max_sequence_d2] = ss[:max_sequence_d2]\n",
    "                x_d2_mask[index, kk, idx] = 1.\n",
    "    \n",
    "    #################3 maxlen_x 都小于 maxlen(100),但每一组maxlen_x都不一样！！\n",
    "    return x, x_mask, x_d1, x_d1_mask, x_d2, x_d2_mask, l, final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#有超参数temp,dim=150/500-->options['attn_pooling'];inst_pred的activation func\n",
    "def milrepre(emb, sequence_mask, instance_mask, keep_prob, is_training, options):\n",
    "    # emb batch,news, sequence,embedding, 32*40*13*100\n",
    "    # sequence_mask batch, news,sequence 32*40*13\n",
    "    # instance_mask batch, news, 32*40\n",
    "    batch = tf.shape(emb)[0]\n",
    "    N = tf.shape(emb)[1]\n",
    "    word = tf.shape(emb)[2]\n",
    "    word_level_inputs = tf.reshape(emb, [batch * N, word, options['dim_word']])\n",
    "    word_level_mask = tf.reshape(sequence_mask, [batch * N, word])\n",
    "    ##TODO average word\n",
    "    word_level_output = tf.reduce_sum(word_level_inputs * tf.expand_dims(word_level_mask, -1), 1) / tf.expand_dims(\n",
    "        tf.reduce_sum(word_level_mask, 1) + 1e-8, 1)\n",
    "    # word_level_output shape is (32*40)*100\n",
    "    att = tf.reshape(word_level_output, [batch, N, options['dim']])\n",
    " \n",
    "    ##TODO MIL bag_representation\n",
    "    instance_temp = tf.layers.dense(word_level_output, options['attn_pooling'], activation=tf.nn.tanh,use_bias=True,kernel_initializer=layers.xavier_initializer(uniform=True,seed=None,dtype=tf.float32),name='inst_temp', reuse=tf.AUTO_REUSE)\n",
    "    if options['use_dropout']:\n",
    "        instance_temp = layers.dropout(instance_temp, keep_prob=keep_prob, is_training=is_training,seed=None)\n",
    "    pred_ = tf.layers.dense(instance_temp, 1, activation=None, use_bias=True,kernel_initializer=layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),name='inst_pred', reuse=tf.AUTO_REUSE)\n",
    "    instance_pred = tf.nn.softmax(pred_)#32*N,1 NOT 32,N,1, float32   \n",
    "    instance_level_input = tf.reshape(instance_pred,[batch,N,1])           \n",
    "    instance_level_input = instance_level_input * tf.expand_dims(instance_mask, -1)  # mask before attention\n",
    "    ##TODO bag_representation\n",
    "    coef = tf.concat([instance_level_input for i in range(options['dim'])], 2)\n",
    "    bag_repre = tf.multiply(coef, att)\n",
    "    bag_repre = tf.reduce_mean(bag_repre, axis=1) #32,100\n",
    "    \n",
    "    return bag_repre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def milrepre_day(emb, sequence_mask, instance_mask, keep_prob, is_training, options):\n",
    "    # emb batch,day,news, sequence,embedding, 32*3*40*13*100\n",
    "    # sequence_mask batch, day, news,sequence 32*3*40*13\n",
    "    # instance_mask batch, day, news, 32*3*40\n",
    "    batch = tf.shape(emb)[0]\n",
    "    day = tf.shape(emb)[1]\n",
    "    N = tf.shape(emb)[2]\n",
    "    word = tf.shape(emb)[3]\n",
    "    word_level_inputs = tf.reshape(emb, [batch * day * N, word, options['dim_word']])\n",
    "    word_level_mask = tf.reshape(sequence_mask, [batch * day * N, word])\n",
    "    instances_level_mask = tf.reshape(instance_mask, [batch * day, N])\n",
    "    \"\"\"##TODO word level LSTM\n",
    "    word_encoder_out = bilstm_filter(word_level_inputs, word_level_mask, keep_prob,\n",
    "                                     prefix='sequence_encode', dim=options['dim'],\n",
    "                                     is_training=is_training)  # output shape: batch*day*news,sequence,2*lstm_units(32*3*40)*12*600\n",
    "    word_encoder_out = tf.concat(word_encoder_out, 2) * tf.expand_dims(word_level_mask, -1)  # mask the output\n",
    "    ##TODO word level attention\n",
    "    word_level_output = attention_v2(word_encoder_out, word_level_mask, name='word_attention', keep=keep_prob, r=10,\n",
    "                                     is_training=is_training)\n",
    "    # word_level_output shape is (32*3*40)*600\n",
    "    if options['use_dropout']:\n",
    "        word_level_output = layers.dropout(word_level_output, keep_prob=keep_prob, is_training=is_training,seed=None)\n",
    "    \"\"\"\n",
    "    ##TODO average word\n",
    "    word_level_output = tf.reduce_sum(word_level_inputs * tf.expand_dims(word_level_mask, -1), 1) / tf.expand_dims(\n",
    "        tf.reduce_sum(word_level_mask, 1) + 1e-8, 1)\n",
    "    # word_level_output shape is (32*3*40)*100\n",
    "    att = tf.reshape(word_level_output, [batch * day, N, options['dim']])  # (32*3),40,100  \n",
    "    \n",
    "    ##TODO MIL bag_representation\n",
    "    instance_temp = tf.layers.dense(word_level_output, options['attn_pooling'], activation=tf.nn.tanh,use_bias=True,kernel_initializer=layers.xavier_initializer(uniform=True,seed=None,dtype=tf.float32),name='inst_temp', reuse=tf.AUTO_REUSE)\n",
    "    if options['use_dropout']:\n",
    "        instance_temp = layers.dropout(instance_temp, keep_prob=keep_prob, is_training=is_training,seed=None)\n",
    "    pred_ = tf.layers.dense(instance_temp, 1, activation=None, use_bias=True,kernel_initializer=layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),name='inst_pred', reuse=tf.AUTO_REUSE)\n",
    "    instance_pred = tf.nn.softmax(pred_)  \n",
    "    instance_level_input = tf.reshape(instance_pred,[batch*day,N,1])   #32*day,N,1, float32    \n",
    "    instance_level_input = instance_level_input * tf.expand_dims(instances_level_mask, -1)  # mask before attention\n",
    "    ##TODO bag_representation\n",
    "    coef = tf.concat([instance_level_input for i in range(options['dim'])], 2)\n",
    "    bag_repre = tf.multiply(coef, att)\n",
    "    bag_repre = tf.reduce_mean(bag_repre, axis=1) #32*day,100\n",
    "    day_level_output = tf.reshape(bag_repre, [batch, day, options['dim']])  # (32*3)*100\n",
    "\n",
    "    return day_level_output #32,3,100\n",
    "\n",
    "\n",
    "##############Questions : instance_level_input = instance_level_input * tf.expand_dims(instances_level_mask, -1)??????????需不需要？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding, options):\n",
    "    \"\"\" Builds the entire computational graph used for training\n",
    "    \"\"\"\n",
    "    # description string: #words x #samples\n",
    "    with tf.device('/gpu:1'):\n",
    "        with tf.variable_scope('input'):\n",
    "            x = tf.placeholder(tf.int64, shape=[None, None, None],\n",
    "                               name='x')  # 3D vector batch,news and sequence(before embedding)40*32*13\n",
    "            x_mask = tf.placeholder(tf.float32, shape=[None, None], name='x_mask')  # mask batch,news\n",
    "            y = tf.placeholder(tf.int64, shape=[None], name='y')\n",
    "            x_d1 = tf.placeholder(tf.int64, shape=[None, None, None, None], name='x_d1')\n",
    "            x_d1_mask = tf.placeholder(tf.float32, shape=[None, None, None], name='x_d1_mask')\n",
    "            x_d2 = tf.placeholder(tf.int64, shape=[None, None, None, None], name='x_d2')\n",
    "            x_d2_mask = tf.placeholder(tf.float32, shape=[None, None, None], name='x_d2_mask')\n",
    "            final_mask = tf.placeholder(tf.float32, shape=[None, None], name='final_mask')\n",
    "            tech = tf.placeholder(tf.float32, shape=[None, None,7], name='technical') #shape is batch time unit\n",
    "            # final_mask shape is day*n_samples\n",
    "            ##TODO important    \n",
    "            keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "            ##TODO important\n",
    "            sequence_mask = tf.cast(tf.abs(tf.sign(x)), tf.float32)  # 3D\n",
    "            sequence_d1_mask = tf.cast(tf.abs(tf.sign(x_d1)), tf.float32)  # 4D\n",
    "            sequence_d2_mask = tf.cast(tf.abs(tf.sign(x_d2)), tf.float32)  # 4D\n",
    "            n_timesteps = tf.shape(x)[0]  # time steps\n",
    "            #n_samples = tf.shape(x)[1]  # n samples\n",
    "            # # word embedding\n",
    "            ##TODO word embedding\n",
    "            emb = tf.nn.embedding_lookup(embedding, x)\n",
    "            emb_d1 = tf.nn.embedding_lookup(embedding, x_d1)\n",
    "            emb_d2 = tf.nn.embedding_lookup(embedding, x_d2)\n",
    "            \n",
    "    with tf.device('/gpu:1'):\n",
    "        # fed into the input of BILSTM from the official document\n",
    "        ##TODO word level LSTM\n",
    "        with tf.name_scope('news'):\n",
    "            att = news(emb, sequence_mask, x_mask, keep_prob, is_training, options)\n",
    "        ##TODO att shape 32*600 att_day1 32*3*600 att_day2 32*4*600\n",
    "        with tf.name_scope('day1'):\n",
    "            att_day1 = days(emb_d1, sequence_d1_mask, x_d1_mask, keep_prob, is_training, options)\n",
    "        # TODO bilstm layers\n",
    "        # Change the time step and batch\n",
    "    with tf.device('/gpu:1'):\n",
    "        with tf.name_scope('day2'):\n",
    "            att_day2 = days(emb_d2, sequence_d2_mask, x_d2_mask, keep_prob, is_training, options)\n",
    "        with tf.name_scope('final'):\n",
    "            final = tf.concat([att_day2, att_day1, tf.expand_dims(att, 1)], 1)\n",
    "            '''if options['use_dropout']:\n",
    "                final = layers.dropout(final, keep_prob=keep_prob, is_training=is_training)\n",
    "            '''\n",
    "            # final shape is 8*32*600\n",
    "            if options['last_layer'] == 'LSTM':\n",
    "                final = bilstm_filter(final, final_mask, keep_prob, prefix='day_lstm', dim=100,\n",
    "                                    is_training=is_training)  # output shape: batch,time_step,2*lstm_unit(concate) 32*7*600\n",
    "                #tech_ind = lstm_filter(tech, tf.ones(shape=[tf.shape(tech)[0],tf.shape(tech)[1]]), keep_prob, prefix='tech_lstm', dim=50,\n",
    "                #                    is_training=is_training)\n",
    "                ##TODO day level attention\n",
    "                att_final = attention_v2(tf.concat(final, 2), final_mask, name='day_attention', keep=keep_prob,r=4,\n",
    "                                is_training=is_training)  # already masked after attention\n",
    "                ##TODO take day lstm average\n",
    "                # att_final = tf.reduce_mean(tf.concat(final,2),1)\n",
    "                # tech_att = tf.reduce_mean(tf.concat(tech_ind,2),1)\n",
    "                ##TODO take the lasts\n",
    "                #tech_att=tech_ind[:,-1,:]\n",
    "                #att_final = tf.concat([att_final,tech_att],axis=1)\n",
    "                logit = tf.layers.dense(att_final, 100, activation=tf.nn.tanh, use_bias=True,\n",
    "                                        kernel_initializer=layers.xavier_initializer(uniform=True, seed=None,\n",
    "                                                                                    dtype=tf.float32),\n",
    "                                        name='ff', reuse=tf.AUTO_REUSE)\n",
    "                # logit = tf.layers.batch_normalization(logit, training=is_training)\n",
    "                # logit=tf.nn.tanh(logit)\n",
    "\n",
    "                '''\n",
    "                # logit1 = tf.reduce_sum(tf.concat(final,2) * tf.expand_dims(final_mask,-1),0) / tf.expand_dims(tf.reduce_sum(final_mask,0),1)\n",
    "                # logit2 = tf.reduce_max(ctx3 * tf.expand_dims(x1_mask,2),0)\n",
    "                '''\n",
    "            if options['last_layer'] == 'CNN':\n",
    "                att_ctx = tf.concat([att_day1, tf.expand_dims(att, 1)], 1)\n",
    "                xavier = layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32)\n",
    "                conv1 = tf.layers.conv1d(att_ctx, filters=options['CNN_filter'],\n",
    "                                        kernel_size=options['CNN_kernel'], padding='same', strides=1,\n",
    "                                        activation=tf.nn.relu, kernel_initializer=xavier, name='conv1')\n",
    "                conv2 = tf.layers.conv1d(final, filters=options['CNN_filter'],\n",
    "                                        kernel_size=options['CNN_kernel'], padding='same',\n",
    "                                        strides=1, activation=tf.nn.relu,\n",
    "                                        kernel_initializer=xavier,\n",
    "                                        name='conv2')\n",
    "\n",
    "                pool1 = tf.layers.max_pooling1d(conv1, pool_size=2, strides=2, padding='same',\n",
    "                                                data_format='channels_last', name='pool1')\n",
    "                pool2 = tf.layers.max_pooling1d(conv2, pool_size=2, strides=2, padding='same',\n",
    "                                                data_format='channels_last', name='pool2')\n",
    "                d1size = math.ceil(options['delay1'] / 2) * options['CNN_filter']\n",
    "                d2size = math.ceil(options['delay2'] / 2) * options['CNN_filter']\n",
    "                pool1_flat = tf.reshape(pool1, [-1, d1size])\n",
    "                pool2_flat = tf.reshape(pool2, [-1, d2size])\n",
    "                cnn_final = tf.concat([att, pool1_flat, pool2_flat], -1)\n",
    "                logit = tf.layers.dense(cnn_final, 300, activation=tf.nn.tanh, use_bias=True,\n",
    "                                        kernel_initializer=layers.xavier_initializer(uniform=True, seed=None,\n",
    "                                                                                    dtype=tf.float32),\n",
    "                                        name='ff', reuse=tf.AUTO_REUSE)\n",
    "                # logit = tf.layers.batch_normalization(logit, training=is_training)\n",
    "                # logit=tf.nn.tanh(logit)\n",
    "\n",
    "            if options['use_dropout']:\n",
    "                logit = layers.dropout(logit, keep_prob=keep_prob, is_training=is_training,seed=None)\n",
    "            pred = tf.layers.dense(logit, 2, activation=None, use_bias=True,\n",
    "                                kernel_initializer=layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "                                name='fout', reuse=tf.AUTO_REUSE)\n",
    "            logger.info('Building f_cost...')\n",
    "            # todo not same\n",
    "            labels = tf.one_hot(y, depth=2, axis=1)\n",
    "            # labels = y\n",
    "            preds = tf.nn.softmax(pred, 1,name='softmax')\n",
    "            # preds = tf.nn.sigmoid(pred)\n",
    "            # pred=tf.reshape(pred,[-1])\n",
    "            cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=labels)\n",
    "            # cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,logits=pred),1)\n",
    "            # cost = -tf.reduce_sum((tf.cast(labels, tf.float32) * tf.log(preds + 1e-8)),axis=1)\n",
    "            #cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=y)\n",
    "        logger.info('Done')\n",
    "        '''\n",
    "        logit1 = tf.reduce_sum(ctx1 * tf.expand_dims(x_mask, 2), 0) / tf.expand_dims(tf.reduce_sum(x_mask, 0), 1)\n",
    "        logit2 = tf.reduce_max(ctx1 * tf.expand_dims(x_mask, 2), 0)\n",
    "        logit = tf.concat([logit1, logit2], 1)\n",
    "        '''\n",
    "\n",
    "        with tf.variable_scope('logging'):\n",
    "            tf.summary.scalar('current_cost', tf.reduce_mean(cost))\n",
    "            tf.summary.histogram('predicted_value', preds)\n",
    "            summary = tf.summary.merge_all()\n",
    "\n",
    "    return is_training, cost, x, x_mask, y, n_timesteps, preds, summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
