{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-NN model averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "validate\n",
      "testing\n",
      "validate_ep\n",
      "val_ac\n",
      "testing_ep\n",
      "test_ac\n",
      "training_ep\n",
      "train_ep\n"
     ]
    }
   ],
   "source": [
    "with open(\"results/s_nn/important_plot.pickle\",'rb') as handles_nn:\n",
    "    dictions_nn = pickle.load(handles_nn)\n",
    "for key, value in dictions_nn.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(diction['val_ac'])#diction['val_ac'] is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "avg_valid_acc_snn = mean(dictions_nn['val_ac'])\n",
    "avg_test_acc_snn = mean(dictions_nn['test_ac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the validation accuracy in s_nn = 0.545511\n",
      "Average of the validation accuracy in s_nn = 0.579205\n"
     ]
    }
   ],
   "source": [
    "print(\"Average of the validation accuracy in s_nn =\", round(avg_valid_acc_snn, 6)) \n",
    "print(\"Average of the test accuracy in s_nn =\", round(avg_test_acc_snn, 6)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-GICF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "validate\n",
      "testing\n",
      "validate_ep\n",
      "val_ac\n",
      "testing_ep\n",
      "test_ac\n",
      "training_ep\n",
      "train_ep\n"
     ]
    }
   ],
   "source": [
    "with open(\"results/mil_s/important_plot.pickle\",'rb') as handlec:\n",
    "    dictionc = pickle.load(handlec)\n",
    "for key, value in dictionc.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_valid_acc_sGICF = mean(dictionc['val_ac'])\n",
    "avg_test_acc_sGICF = mean(dictionc['test_ac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the validation accuracy in sGICF = 0.545455\n",
      "Average of the validation accuracy in sGICF = 0.579583\n"
     ]
    }
   ],
   "source": [
    "print(\"Average of the validation accuracy in sGICF =\", round(avg_valid_acc_sGICF, 6)) \n",
    "print(\"Average of the test accuracy in sGICF =\", round(avg_test_acc_sGICF, 6)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/s_lstm/important_plot.pickle\",'rb') as handled:\n",
    "    dictiond = pickle.load(handled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_valid_acc_slstm = mean(dictiond['val_ac'])\n",
    "avg_test_acc_slstm = mean(dictiond['test_ac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the validation accuracy in s-lstm = 0.508523\n",
      "Average of the test accuracy in s-lstm = 0.553602\n"
     ]
    }
   ],
   "source": [
    "print(\"Average of the validation accuracy in s-lstm =\", round(avg_valid_acc_slstm, 6)) \n",
    "print(\"Average of the test accuracy in s-lstm =\", round(avg_test_acc_slstm, 6)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIL-rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/mil_rep/important_plot.pickle\",'rb') as handlef:\n",
    "    dictionf = pickle.load(handlef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## According to the figures, after about epoch 210 began to overfit, therefore we obstain the first 210/200 epochs\n",
    "avg_valid_acc_milrep = mean(dictionf['val_ac'][:200])\n",
    "avg_test_acc_milrep = mean(dictionf['test_ac'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the validation accuracy in mil-rep = 0.538409\n",
      "Average of the test accuracy in mil-rep = 0.581222\n"
     ]
    }
   ],
   "source": [
    "print(\"Average of the validation accuracy in mil-rep =\", round(avg_valid_acc_milrep, 6)) \n",
    "print(\"Average of the test accuracy in mil-rep =\", round(avg_test_acc_milrep, 6)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATT-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/att_lstm1/important_plot.pickle\",'rb') as handlee:\n",
    "    dictione = pickle.load(handlee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_valid_acc_attlstm = mean(dictione['val_ac'])\n",
    "avg_test_acc_attlstm = mean(dictione['test_ac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the validation accuracy in attlstm = 0.516846\n",
      "Average of the test accuracy in attlstm = 0.579072\n"
     ]
    }
   ],
   "source": [
    "print(\"Average of the validation accuracy in attlstm =\", round(avg_valid_acc_attlstm, 6)) \n",
    "print(\"Average of the test accuracy in attlstm =\", round(avg_test_acc_attlstm, 6)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
