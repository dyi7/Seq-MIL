{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy \n",
    "numpy.random.seed(1)\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "from tensorflow import logging  as log\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from collections import OrderedDict\n",
    "from data_iteratorMIL import TextIterator\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import pprint\n",
    "import pdb\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _s(pp, name):  # add perfix\n",
    "    return '{}_{}'.format(pp, name)\n",
    "\n",
    "\n",
    "def load_params(path, params):\n",
    "    pp = numpy.load(path)\n",
    "    for kk, vv in params.iteritems():\n",
    "        if kk not in pp:\n",
    "            warnings.warn('{} is not in the archive'.format(kk))\n",
    "            continue\n",
    "        params[kk] = pp[kk]\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant=1):\n",
    "    low = -constant * numpy.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * numpy.sqrt(6.0 / (fan_in + fan_out))\n",
    "    W = numpy.random.uniform(low=low, high=high, size=(fan_in, fan_out))\n",
    "    return W.astype('float32')\n",
    "\n",
    "\n",
    "def ortho_weight(ndim):  # used by norm_weight below\n",
    "    \"\"\"\n",
    "    Random orthogonal weights\n",
    "    Used by norm_weights(below), in which case, we\n",
    "    are ensuring that the rows are orthogonal\n",
    "    (i.e W = U \\Sigma V, U has the same\n",
    "    # of rows, V has the same # of cols)\n",
    "    \"\"\"\n",
    "    W = numpy.random.randn(ndim, ndim)\n",
    "    u, s, v = numpy.linalg.svd(W)\n",
    "    return u.astype('float32')\n",
    "\n",
    "\n",
    "def norm_weight(nin, nout=None, scale=0.01, ortho=True):\n",
    "    \"\"\"\n",
    "    Random weights drawn from a Gaussian\n",
    "    \"\"\"\n",
    "    if nout is None:\n",
    "        nout = nin\n",
    "    if nout == nin and ortho:\n",
    "        W = ortho_weight(nin)\n",
    "    else:\n",
    "        # W = numpy.random.uniform(-0.5,0.5,size=(nin,nout))\n",
    "        W = scale * numpy.random.randn(nin, nout)\n",
    "    return W.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_v2(input, mask, name='attention', nin=100, keep=1.0, r=10, is_training=True):\n",
    "    # input is batch,time_step,hidden_state (32*40)*13*600 mask (32*40)*13\n",
    "    # hidden layer is:batch,hidden_shape,attention_hidden_size (32*40)*13*1200 or (32*40)*13*600\n",
    "    # attention shape after squeeze is (32*40)*13, # batch,time_step,attention_size (32*40)*13*1\n",
    "    with tf.variable_scope(name_or_scope=name, reuse=tf.AUTO_REUSE):\n",
    "        masks = tf.stack([mask] * r, -1)  # copy r time for filling (32*40)*13*r\n",
    "        iden = tf.eye(r, batch_shape=[tf.shape(input)[0]])  # an identity matrix (32*40)*13*13\n",
    "        hidden = tf.layers.dense(input, nin / 2, activation=tf.nn.tanh, use_bias=False,\n",
    "                                kernel_initializer=layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "                                name='hidden', reuse=tf.AUTO_REUSE)\n",
    "        attention = tf.layers.dense(hidden, r, activation=None, use_bias=False,\n",
    "                                    kernel_initializer=layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "                                    name='out',\n",
    "                                    reuse=tf.AUTO_REUSE)  # attention shape is 32*40*r\n",
    "        padding = tf.fill(tf.shape(attention), float('-1e8'))  # float('-inf')\n",
    "        attention = tf.where(tf.equal(masks, 0.), padding, attention)  # fill 0 with a small number for softmax\n",
    "        attention = tf.nn.softmax(attention, 1,name='softmax') * masks  # (32*40)*13*r #mask the attention here is not really neccesary,\n",
    "        penalty = tf.norm((tf.matmul(tf.transpose(attention, [0, 2, 1]), attention) - iden), ord='fro',\n",
    "                        axis=(-2, -1))  # the Frobenius norm penalty 32 dimension\n",
    "        outputs = tf.matmul(tf.transpose(attention, [0, 2, 1]), input)  # transpose to batch,hidden,time_step\n",
    "        ##TODO average sentence attention\n",
    "        #results = tf.reduce_mean(outputs, 1)  # average sentence attention\n",
    "        ##TODO attention over attention\n",
    "        \n",
    "        over_hidden = tf.layers.dense(outputs, nin, activation=tf.nn.tanh, use_bias=False,\n",
    "                                kernel_initializer=layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "                                name='over_attention_hidden', reuse=tf.AUTO_REUSE)\n",
    "        over_attention = tf.layers.dense(over_hidden, 1, activation=None, use_bias=False,\n",
    "                                    kernel_initializer=layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "                                    name='over_attention_out',\n",
    "                                    reuse=tf.AUTO_REUSE)\n",
    "        over_attention = tf.nn.softmax(over_attention, 1,name='over_attention_softmax')\n",
    "        results = tf.reduce_sum(outputs * over_attention, axis=1)  # 32*600\n",
    "        \n",
    "    return results  # result shape is batch, hidden_unit (32*40)*600\n",
    "\n",
    "\n",
    "def lstm_filter(input, mask, keep_prob, prefix='lstm', dim=50, is_training=True):\n",
    "    with tf.variable_scope(name_or_scope=prefix, reuse=tf.AUTO_REUSE):\n",
    "        sequence = tf.cast(tf.reduce_sum(mask, 1), tf.int32)\n",
    "        lstm_fw_cell = rnn.LSTMCell(dim, forget_bias=0.0, initializer=tf.orthogonal_initializer(), state_is_tuple=True)\n",
    "        keep_rate = tf.cond(is_training is not False and keep_prob < 1, lambda: 0.8, lambda: 1.0)\n",
    "        cell_dp_fw = rnn.DropoutWrapper(cell=lstm_fw_cell, output_keep_prob=keep_rate)\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell_dp_fw, input, sequence_length=sequence,swap_memory=False,\n",
    "                                       dtype=tf.float32)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def bilstm_filter(input, mask, keep_prob, prefix='lstm', dim=50, is_training=True):\n",
    "    with tf.variable_scope(name_or_scope=prefix, reuse=tf.AUTO_REUSE):\n",
    "        sequence = tf.cast(tf.reduce_sum(mask, 1), tf.int32)\n",
    "        lstm_fw_cell = rnn.LSTMBlockCell(dim, forget_bias=1.0)# initializer=tf.orthogonal_initializer(), state_is_tuple=True\n",
    "        # back directions\n",
    "        lstm_bw_cell = rnn.LSTMBlockCell(dim, forget_bias=1.0)\n",
    "        keep_rate = tf.cond(is_training is not False and keep_prob < 1, lambda: 0.8, lambda: 1.0)\n",
    "        cell_dp_fw = rnn.DropoutWrapper(cell=lstm_fw_cell, output_keep_prob=keep_rate)\n",
    "        cell_dp_bw = rnn.DropoutWrapper(cell=lstm_bw_cell, output_keep_prob=keep_rate)\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_dp_fw, cell_dp_bw, input, sequence_length=sequence,swap_memory=False,\n",
    "                                                     dtype=tf.float32)  # batch major\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_level_input = word_level_inputs * tf.expand_dims(word_level_mask, -1)  # mask before attention\n",
    "#word_level_output = cnn_sent(word_level_input, options, name='news_cnn')\n",
    "\n",
    "def cnn_sent(input, options, name='instances_cnn'):\n",
    "    \"input shape: 32*N, word, options['dim_word'], output shape:32*N, options['dim_word']\"\n",
    "    with tf.variable_scope(name_or_scope=name, reuse=tf.AUTO_REUSE):\n",
    "        conv1 = tf.layers.conv1d(input, filters=options['CNN_filter'],\n",
    "                                 kernel_size=3, padding='same', strides=1,\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 kernel_initializer=layers.xavier_initializer(uniform=True, seed=None,\n",
    "                                                                              dtype=tf.float32), name='conv1')\n",
    "        conv2 = tf.layers.conv1d(input, filters=options['CNN_filter'],\n",
    "                                 kernel_size=4, padding='same', strides=1,\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 kernel_initializer=layers.xavier_initializer(uniform=True, seed=None,\n",
    "                                                                              dtype=tf.float32), name='conv2')\n",
    "        conv3 = tf.layers.conv1d(input, filters=options['CNN_filter'],\n",
    "                                 kernel_size=5, padding='same', strides=1,\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 kernel_initializer=layers.xavier_initializer(uniform=True, seed=None,\n",
    "                                                                              dtype=tf.float32), name='conv3')                                                                                                                                         \n",
    "        concat=tf.concat([conv1,conv2,conv3],-1)\n",
    "        gmp = tf.reduce_max(concat, reduction_indices=1, name='gmp')##global max pooling\n",
    "\n",
    "    return gmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(options, worddicts):\n",
    "    params = OrderedDict()\n",
    "    # embedding\n",
    "    params['Wemb'] = norm_weight(options['n_words'], options['dim_word'])\n",
    "    # read embedding from GloVe\n",
    "    if options['embedding']:\n",
    "        with open(options['embedding'], 'r') as f:\n",
    "            for line in f:\n",
    "                tmp = line.split()\n",
    "                word = tmp[0]\n",
    "                vector = tmp[1:]\n",
    "                if word in worddicts and worddicts[word] < options['n_words']:\n",
    "                    try:\n",
    "                        params['Wemb'][worddicts[word], :] = vector\n",
    "                        # encoder: bidirectional RNN\n",
    "                    except ValueError as e:\n",
    "                        print(str(e))\n",
    "    return params\n",
    "\n",
    "\n",
    "def word_embedding(options, params):\n",
    "    embeddings = tf.get_variable(\"embeddings\", shape=[options['n_words'], options['dim_word']],\n",
    "                                 initializer=tf.constant_initializer(numpy.array(\n",
    "                                     params['Wemb'])))  # tf.constant_initializer(numpy.array(params['Wemb']))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#att = news(emb(4维), sequence_mask(3维0-1阵), x_mask(2维), keep_prob, is_training, options)\n",
    "def sentence_att_emb(emb, sequence_mask, instance_mask, keep_prob, is_training, options):\n",
    "    # (4维)emb.shape = batch,news, sequence,embedding, 32*40*13*300(emb_size)\n",
    "    # (3维0-1阵)sequence_mask.shape = batch, news,sequence 32*40*13\n",
    "    # (2维)instance_mask。shape = batch, news, 32*40  #news_mask\n",
    "    batch = tf.shape(emb)[0] #32\n",
    "    instance_len = tf.shape(emb)[1] #40 N instances in a group\n",
    "    word = tf.shape(emb)[2]  #13\n",
    "    word_level_inputs = tf.reshape(emb, [batch * instance_len, word, options['dim_word']])\n",
    "    word_level_mask = tf.reshape(sequence_mask, [batch * instance_len, word])\n",
    "    ##TODO word level LSTM\n",
    "    \n",
    "    word_encoder_out = bilstm_filter(word_level_inputs, word_level_mask, keep_prob,\n",
    "                                     prefix='sequence_encode', dim=options['dim'],\n",
    "                                     is_training=is_training)  # output shape: batch*news,sequence,2*lstm_units(32*40)*12*600\n",
    "    word_encoder_out = tf.concat(word_encoder_out, 2) * tf.expand_dims(word_level_mask, -1)  \n",
    "    # mask the output，按axis=2维度(sequence_word维度)concat --> 一个句子里不同字数  降维？\n",
    "    # Why mask?! ---> 使得每个sentence句子  字数对齐！！ \n",
    "    ################################### TODO word-attention ####################################\n",
    "    word_level_output = attention_v2(word_encoder_out, word_level_mask, name='word_attention', keep=keep_prob,r=10,\n",
    "                                     is_training=is_training)\n",
    "    \n",
    "   \n",
    "    if options['use_dropout']:\n",
    "        word_level_output = layers.dropout(word_level_output, keep_prob=keep_prob, is_training=is_training,seed=None)\n",
    "    \n",
    "    # return word_level_output\n",
    "    # word_level_output shape is (32*40)*600 #lstm_hidden_state =600\n",
    "    \n",
    "    instance_level_input = tf.reshape(word_level_output, [batch, instance_len, 2*options['dim']])  # 32*40*600\n",
    "    #instance_level_input = instance_level_input * tf.expand_dims(instance_mask, -1)  # mask before attention  32*40\n",
    "    ##TODO average of its instances\n",
    "    #instance_level_output = tf.reduce_sum(instance_level_input * tf.expand_dims(instance_mask, -1), 1) / tf.expand_dims(tf.reduce_sum(instance_mask, 1) + 1e-8, 1)\n",
    "    ##TODO Primary instance\n",
    "    # instance_level_output = tf.reduce_max(instance_level_input * tf.expand_dims(instance_mask, -1), 1)  #.eval()\n",
    "    # shape is 32*600\n",
    "    return instance_level_input #32*40*600\n",
    "\n",
    "def sentence_avg_emb(emb, sequence_mask, instance_mask, keep_prob, is_training, options):\n",
    "    # (4维)emb.shape = batch,news, sequence,embedding, 32*40*13*300(emb_size)\n",
    "    # (3维0-1阵)sequence_mask.shape = batch, news,sequence 32*40*13\n",
    "    # (2维)instance_mask。shape = batch, news, 32*40  #news_mask\n",
    "    batch = tf.shape(emb)[0] #32\n",
    "    instance_len = tf.shape(emb)[1] #40 N instances in a group\n",
    "    word = tf.shape(emb)[2]  #13\n",
    "    word_level_inputs = tf.reshape(emb, [batch * instance_len, word, options['dim_word']])\n",
    "    word_level_mask = tf.reshape(sequence_mask, [batch * instance_len, word])\n",
    "    ################################### TODO average word #####################################\n",
    "    word_level_output = tf.reduce_sum(word_level_inputs * tf.expand_dims(word_level_mask, -1), 1) / tf.expand_dims(tf.reduce_sum(word_level_mask, 1) + 1e-8, 1)  \n",
    "    # word_level_output shape is (32*40)*100('dim_word')\n",
    "   \n",
    "    if options['use_dropout']:\n",
    "        word_level_output = layers.dropout(word_level_output, keep_prob=keep_prob, is_training=is_training,seed=None)\n",
    "    \n",
    "    instance_level_input = tf.reshape(word_level_output, [batch, instance_len, 2*options['dim']])  # 32*40*100\n",
    "    #instance_level_input = instance_level_input * tf.expand_dims(instance_mask, -1)  # mask before attention\n",
    "    return instance_level_input #(32*40)*100('dim_word')\n",
    "\n",
    "\n",
    "def sentence_cnn_emb(emb, sequence_mask, instance_mask, keep_prob, is_training, options):\n",
    "    # (4维)emb.shape = batch,news, sequence,embedding, 32*40*13*300(emb_size)\n",
    "    # (3维0-1阵)sequence_mask.shape = batch, news,sequence 32*40*13\n",
    "    # (2维)instance_mask。shape = batch, news, 32*40  #news_mask\n",
    "    batch = tf.shape(emb)[0] #32\n",
    "    instance_len = tf.shape(emb)[1] #40 N instances in a group\n",
    "    word = tf.shape(emb)[2]  #13\n",
    "    word_level_inputs = tf.reshape(emb, [batch * instance_len, word, options['dim_word']])\n",
    "    word_level_mask = tf.reshape(sequence_mask, [batch * instance_len, word])\n",
    "    # Why mask?! ---> 使得每个sentence句子  字数对齐！！ \n",
    "    ################################# TODO CNN encode  ###########################################\n",
    "    word_level_input = word_level_inputs * tf.expand_dims(word_level_mask, -1)  # mask before cnn\n",
    "    word_level_output = cnn_sent(word_level_input, options, name='news_cnn')  \n",
    "    # shape = (32*40)*100(options['dim_word'])\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        word_level_output = layers.dropout(word_level_output, keep_prob=keep_prob, is_training=is_training,seed=None)\n",
    "        \n",
    "    instance_level_input = tf.reshape(word_level_output, [batch, instance_len, 2*options['dim']])  # 32*40*100\n",
    "    #instance_level_input = instance_level_input * tf.expand_dims(instance_mask, -1)  # mask before attention  32*40\n",
    "    ##TODO average of its instances\n",
    "    #instance_level_output = tf.reduce_sum(instance_level_input * tf.expand_dims(instance_mask, -1), 1) / tf.expand_dims(tf.reduce_sum(instance_mask, 1) + 1e-8, 1)\n",
    "    ##TODO Primary instance\n",
    "    # instance_level_output = tf.reduce_max(instance_level_input * tf.expand_dims(instance_mask, -1), 1)  #.eval()\n",
    "    # shape is 32*600\n",
    "    return instance_level_input #32*40*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x, y in train:\n",
    "#    data_x, data_x_mask, data_y = prepare_data(x,y,model_options,maxlen=maxlen)\n",
    "# (32, 11, 16) (32, 11) (32,)\n",
    "\n",
    "def prepare_data(sequence, labels, options, maxlen=None, max_word=100):\n",
    "    # length = [len(s) for s in sequence]\n",
    "    length = []   #一天里面 number of news\n",
    "    for i in sequence: #x,one batch里面one day\n",
    "        length.append(len(i))\n",
    "    if maxlen is not None:  # max length is the news level\n",
    "        new_sequence = []\n",
    "        new_lengths = []\n",
    "        \n",
    "        for l, s in zip(length, sequence):\n",
    "            if l < maxlen:\n",
    "                new_sequence.append(s)\n",
    "                new_lengths.append(l)\n",
    "\n",
    "        length = new_lengths  # This step is to filter the sentence which length is bigger\n",
    "        sequence = new_sequence  # than the max length. length means number of news. sequence means \n",
    "        ##TODO need to be careful, set the max length bigger to avoid bug\n",
    "        if len(length) < 1:\n",
    "            return None, None, None, None, None, None, None, None\n",
    "        \n",
    "    maxlen_x = numpy.max(length)  # max time step   <100(maxlen)\n",
    "    n_samples = len(sequence)  # number of samples == batch\n",
    "    max_sequence = max(len(j) for i in sequence for j in i)  # find the sequence max length\n",
    "    max_sequence = max_word if max_sequence > max_word else max_sequence  # shrink the data size\n",
    "    ##TODO for x\n",
    "    x = numpy.zeros((n_samples, maxlen_x, max_sequence)).astype('int64')  #(32, 11, 16)\n",
    "    x_mask = numpy.zeros((n_samples, maxlen_x)).astype('float32')  #(32, 11)\n",
    "    ##TODO for label\n",
    "    l = numpy.zeros((n_samples,)).astype('int64')\n",
    "    for index, (i, ll) in enumerate(zip(sequence, labels)):  # batch size\n",
    "        l[index] = ll\n",
    "        for idx, ss in enumerate(i):  # time step\n",
    "            # x[idx, index, :sequence_length[idx]] = ss\n",
    "            if len(ss) < max_sequence:\n",
    "                x[index, idx, :len(ss)] = ss\n",
    "            else:\n",
    "                x[index, idx, :max_sequence] = ss[:max_sequence]\n",
    "            x_mask[index, idx] = 1.\n",
    "    '''\n",
    "    haha = numpy.absolute(numpy.sign(x))\n",
    "    hehe = numpy.absolute(numpy.sign(x_d1))\n",
    "    jiji = numpy.absolute(numpy.sign(x_d2))\n",
    "    '''\n",
    "    #################3 maxlen_x 都小于 maxlen(100),但每一组maxlen_x都不一样！！\n",
    "    return x, x_mask, l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance(x):  \n",
    "    \"x.shape: (32,N,emb_size), then the function is to get a batch matrix (32,N,N)\"\n",
    "    #ba = x.get_shape().as_list()[0]\n",
    "    N = x.get_shape().as_list()[1]\n",
    "    # expand dim\n",
    "    x_expand = tf.expand_dims(x,2) #32,N,1,emb_size\n",
    "    xT_expand = tf.expand_dims(x,3) #32,N,emb_size,1\n",
    "    # tf.tile--> make copy\n",
    "    x_tile = tf.tile(x_expand,[1,1,N,1])  #32,N,N,emb_size\n",
    "    xT_tile = tf.tile(xT_expand,[1,1,1,N])  #32,N,emb_size,N\n",
    "    # transpose\n",
    "    l = tf.transpose(xT_tile,[0,3,1,2])  #32,N,N,emb_size\n",
    "    ## Calculate the distance\n",
    "    eu_dis = tf.reduce_sum(tf.square(x_tile-l),3)  #32,N,N\n",
    "    kernel_dis = tf.exp(-eu_dis)  #values between(0,1)\n",
    "    \n",
    "    return kernel_dis\n",
    "\n",
    "def instance_diff(l):\n",
    "    \"The shape of input x is:(32,N),with values between(0,1), the shape of output (instance_pred(i)-instance_pred(j))^2 is (32,N,N)\"\n",
    "    N = l.get_shape().as_list()[-1]\n",
    "    l_expand = tf.expand_dims(l,1)  #32,1,N\n",
    "    lT_expand = tf.expand_dims(l,2) #32,N,1\n",
    "    l_tile = tf.tile(l_expand,[1,N,1])\n",
    "    lT_tile = tf.tile(lT_expand,[1,1,N])\n",
    "    dist = tf.square(l_tile-lT_tile)   #32,N,N\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def instance_pred(input,name = 'MIL'):\n",
    "    with tf.variable_scope(name_or_scope=name, reuse=tf.AUTO_REUSE):\n",
    "        mini_batch = tf.shape(input)[0]  #32\n",
    "        N = tf.shape(input)[1] #N\n",
    "        emb_size = tf.shape(input)[2]  #600/100\n",
    "        a_input = tf.reshape(input,[mini_batch*N, emb_size])  #32*N,600\n",
    "\n",
    "        theta = tf.get_variable('theta', [emb_size, 1],initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        #theta = tf.get_variable('theta', shape=[emb_size, 1], initializer = tf.truncated_normal_initializer(stddev=0.1, seed=1))\n",
    "\n",
    "        ##TODO make instances prediction through softmax(sigmoid) function\n",
    "        inst_pred = tf.sigmoid(tf.matmul(a_input,theta))  #32*N,1\n",
    "        L = tf.reshape(inst_pred,[mini_batch,N])  #32,N\n",
    "        #print(inst_pred)\n",
    "        ##TODO make group prediction through average instance predictions\n",
    "        group_ = tf.reduce_mean(L,1)  # Do the instance_pred average  \n",
    "        group_pred = tf.cast(tf.ceil(group_-0.5),tf.int32)    #32\n",
    "        \n",
    "    return group_pred\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding, options):\n",
    "    \"\"\" Builds the entire computational graph used for training\n",
    "    \"\"\"\n",
    "    # description string: #words x #samples\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope('input'):\n",
    "            x = tf.placeholder(tf.int64, shape=[None, None, None],\n",
    "                               name='x')  # 3D vector batch,N and instances(before embedding)40*32*13\n",
    "            x_mask = tf.placeholder(tf.float32, shape=[None, None], name='x_mask')  # mask batch,N\n",
    "            # x_simil = tf.placeholder(tf.float32,shape = [None, None, None], name = 'x_simil') #similarity matrix,   batch,N,N\n",
    "            # l = tf.placeholder(tf.float32,shape = [None, None],name = 'l') #instance_pred, batch,N\n",
    "            # l_diff = tf.placeholder(tf.float32,shape = [None, None, None], name = 'l_diff') #instance_pred_diff,  batch,N,N\n",
    "            #l_der = tf.placeholder(tf.float32,shape = [None, None, None], name = 'l_der')  #derivative\n",
    "            y = tf.placeholder(tf.int64, shape=[None], name='y') #group actual\n",
    "            ##TODO important    \n",
    "            keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "            ##TODO important\n",
    "            sequence_mask = tf.cast(tf.abs(tf.sign(x)), tf.float32)  # 3D\n",
    "            n_timesteps = tf.shape(x)[0]  # time steps\n",
    "            #n_samples = tf.shape(x)[1]  # n samples\n",
    "            # # word embedding\n",
    "            ##TODO word embedding\n",
    "            emb = tf.nn.embedding_lookup(embedding, x)\n",
    "            '''if options['use_dropout']:\n",
    "            emb = layers.dropout(emb, keep_prob=keep_prob, is_training=is_training)\n",
    "            '''\n",
    "    with tf.device('/gpu:0'):\n",
    "        # fed into the input of BILSTM from the official document\n",
    "        ##TODO word level LSTM\n",
    "        with tf.name_scope('instance_prediction'):\n",
    "            att = sentence_att_emb(emb, sequence_mask, x_mask, keep_prob, is_training, options)\n",
    "            #avg = sentence_avg_emb(emb, sequence_mask, x_mask, keep_prob, is_training, options)\n",
    "            #cnn_ = sentence_cnn_emb(emb, sequence_mask, x_mask, keep_prob, is_training, options)\n",
    "            ##TODO att shape 32*40*600, avg/cnn_  32*40*100\n",
    "            mini_batch = tf.shape(att)[0]  #32\n",
    "            N = tf.shape(att)[1] #N\n",
    "            emb_size = tf.shape(att)[2]  #600/100\n",
    "            D = att.get_shape().as_list()[-1]\n",
    "            att_input = tf.reshape(att,[mini_batch*N, emb_size])  #32*N,600\n",
    "            \n",
    "            theta = tf.Variable(tf.random_normal_initializer(shape=[D, 1]),stddev=0.1)\n",
    "            #theta = tf.get_variable('theta', shape=[D, 1], initializer = tf.truncated_normal_initializer(stddev=0.1, seed=1))\n",
    "            \n",
    "            ##TODO make instances prediction through softmax(sigmoid) function\n",
    "            inst_pred = tf.sigmoid(tf.matmul(att_input,theta))  #32*N,1\n",
    "            L = tf.reshape(inst_pred,[mini_batch,N])  #32,N\n",
    "            #print(inst_pred)\n",
    "            ##TODO make group prediction through average instance predictions\n",
    "            group_ = tf.reduce_mean(L,1)  # Do the instance_pred average  \n",
    "            group_pred = tf.cast(tf.ceil(group_-0.5),tf.int32)    #32\n",
    "            print(group_)\n",
    "            print('*'*80)\n",
    "            print(group_pred)\n",
    "            \n",
    "            ##TODO new cost\n",
    "            logger.info('Building f_cost...')\n",
    "            x_simil = Euclidean_distance(att)  #32,N,N   有placeholder\n",
    "            l_diff = instance_diff(L) #32,N,N   有placeholder\n",
    "            simil_cost = tf.reduce_mean(tf.multiply(x_simil,l_diff),[1,2])  #32\n",
    "            group_cost = tf.square(y-group_pred)  #32\n",
    "            total_cost = simil_cost + options['alpha_balance']*group_cost  #32,1]\n",
    "            cost = tf.reshape(total_cost,(1,-1))  #1,32\n",
    "            \n",
    "            \n",
    "            \"\"\"pred = tf.layers.dense(logit, 2, activation=None, use_bias=True,\n",
    "                                kernel_initializer=layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32),\n",
    "                                name='fout', reuse=tf.AUTO_REUSE)#32,2\n",
    "            labels = tf.one_hot(y, depth=2, axis=1)#32,2\n",
    "            preds = tf.nn.softmax(pred, 1,name='softmax')  #32,2\n",
    "            cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=labels)  #1,32\"\"\"\n",
    "\n",
    "        logger.info('Done')\n",
    "\n",
    "\n",
    "        with tf.variable_scope('logging'):\n",
    "            tf.summary.scalar('current_cost', tf.reduce_mean(cost))\n",
    "            tf.summary.histogram('predicted_value', preds)\n",
    "            summary = tf.summary.merge_all()\n",
    "\n",
    "    return is_training, cost, x, x_mask, y, n_timesteps, group_pred, summary  \n",
    "    #return is_training, cost, x, x_mask, y, n_timesteps, preds, summary\n",
    "    # preds:(32,2)  (0.2228,1-0.228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_acc, valid_loss,valid_final_result = predict_pro_acc(sess, cost, prepare_data, model_options, valid, maxlen,\n",
    "                                                            #correct_pred, pred, summary, eidx, is_training, train_op,loss_plot,\n",
    "                                                            #validate_writer,validate=True)\n",
    "def predict_pro_acc(sess, cost, prepare_data, model_options, iterator, maxlen, correct_pred, group_pred, summary, eidx,\n",
    "                    is_training,train_op, plot=None,writer=None,validate=False):\n",
    "    #pred-->group_pred\n",
    "    # fo = open(_s(prefix,'pre.txt'), \"w\")\n",
    "    num = 0\n",
    "    valid_acc = 0\n",
    "    total_cost = 0\n",
    "    loss = 0\n",
    "    result = 0\n",
    "    final_result=[]\n",
    "    #sess.add_tensor_filter(\"val_test_spot\")\n",
    "    for x_sent, y_sent in iterator:\n",
    "        num += len(x_sent)\n",
    "        data_x, data_x_mask, data_y = prepare_data(x_sent,y_sent,model_options,maxlen=maxlen)\n",
    "\n",
    "        loss, result, preds = sess.run([cost, correct_pred, group_pred],  #pred(32,2)-->group_pred(32,)\n",
    "                                       feed_dict={'input/x:0': data_x, 'input/x_mask:0': data_x_mask,\n",
    "                                                  'input/y:0': data_y, \n",
    "                                                  'input/keep_prob:0': 1.0,\n",
    "                                                  'input/is_training:0': is_training})\n",
    "        valid_acc += result.sum()\n",
    "        total_cost += loss.sum()\n",
    "        if plot is not None:\n",
    "            if validate is True:\n",
    "                plot['validate'].append(loss.sum()/len(x_sent))\n",
    "            else:\n",
    "                plot['testing'].append(loss.sum()/len(x_sent))\n",
    "        final_result.extend(result.tolist())\n",
    "    final_acc = 1.0 * valid_acc / num\n",
    "    final_loss = 1.0 * total_cost / num\n",
    "    # if writer is not None:\n",
    "    #    writer.add_summary(test_summary, eidx)\n",
    "\n",
    "    # print result,preds,loss,result_\n",
    "    print(preds, result, num)\n",
    "\n",
    "    return final_acc, final_loss,final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        dim_word=100,  # word vector dimensionality\n",
    "        dim=50, #100,  # the number of GRU units\n",
    "        encoder='lstm',  # encoder model\n",
    "        decoder='lstm',  # decoder model\n",
    "        patience=10,  # early stopping patience\n",
    "        max_epochs=5000,\n",
    "        finish_after=10000000,  # finish after this many updates\n",
    "        decay_c=0.,  # L2 regularization penalty\n",
    "        clip_c=-1.,  # gradient clipping threshold\n",
    "        lrate=0.0004,  # learning rate\n",
    "        n_words=100000,  # vocabulary size\n",
    "        n_words_lemma=100000,\n",
    "        maxlen=100,  # maximum length of the description\n",
    "        alpha_balance=0.04,\n",
    "        optimizer='adam',\n",
    "        batch_size=32,\n",
    "        valid_batch_size=32,\n",
    "        save_model='results/MIL/',\n",
    "        saveto='MIL.npz',\n",
    "        dispFreq=100,\n",
    "        validFreq=1000,\n",
    "        saveFreq=1000,  # save the parameters after every saveFreq updates\n",
    "        use_dropout=False,\n",
    "        reload_=False,\n",
    "        verbose=False,  # print verbose information for debug but slow speed\n",
    "        types='title',\n",
    "        cut_word=False,\n",
    "        cut_news=False,\n",
    "        keep_prob = 0.8,\n",
    "        datasets=[],\n",
    "        valid_datasets=[],\n",
    "        test_datasets=[],\n",
    "        tech_data = [],\n",
    "        dictionary=[],\n",
    "        kb_dicts=[],\n",
    "        embedding='',  # pretrain embedding file, such as word2vec, GLOVE\n",
    "        dim_kb=5,\n",
    "        RUN_NAME=\"histogram_visualization\",\n",
    "        wait_N=10\n",
    "):\n",
    "    logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s: %(name)s: %(levelname)s: %(message)s\",\n",
    "                        filename='results/MIL/log_result.txt')\n",
    "    # Model options\n",
    "    model_options = locals().copy()\n",
    "\n",
    "    with open(dictionary, 'rb') as f:\n",
    "        worddicts = pkl.load(f)\n",
    "\n",
    "    logger.info(\"Loading knowledge base ...\")\n",
    "\n",
    "    # reload options\n",
    "    if reload_ and os.path.exists(saveto):\n",
    "        logger.info(\"Reload options\")\n",
    "        with open('%s.pkl' % saveto, 'rb') as f:\n",
    "            model_options = pkl.load(f)\n",
    "\n",
    "    logger.debug(pprint.pformat(model_options))\n",
    "    \n",
    "    logger.info(\"Loading data\")\n",
    "    train = TextIterator(datasets[0], datasets[1],\n",
    "                         dict=dictionary,\n",
    "                         types=types,\n",
    "                         n_words=n_words,\n",
    "                         batch_size=batch_size,\n",
    "                         cut_word=cut_word,\n",
    "                         cut_news=cut_news,\n",
    "                         shuffle=True, shuffle_sentence=False,quiet=False)\n",
    "    train_valid = TextIterator(datasets[0], datasets[1],\n",
    "                               dict=dictionary,\n",
    "                               types=types,\n",
    "                               n_words=n_words,\n",
    "                               batch_size=valid_batch_size,\n",
    "                               cut_word=cut_word,\n",
    "                               cut_news=cut_news,\n",
    "                               shuffle=False, shuffle_sentence=False,quiet=False)\n",
    "    valid = TextIterator(valid_datasets[0], valid_datasets[1],\n",
    "                         dict=dictionary,\n",
    "                         types=types,\n",
    "                         n_words=n_words,\n",
    "                         batch_size=valid_batch_size,\n",
    "                         cut_word=cut_word,\n",
    "                         cut_news=cut_news,\n",
    "                         shuffle=False, shuffle_sentence=False,quiet=False)\n",
    "    test = TextIterator(test_datasets[0], test_datasets[1],\n",
    "                        dict=dictionary,\n",
    "                        types=types,\n",
    "                        n_words=n_words,\n",
    "                        batch_size=valid_batch_size,\n",
    "                        cut_word=cut_word,\n",
    "                        cut_news=cut_news,\n",
    "                        shuffle=False, shuffle_sentence=False,quiet=False)\n",
    "\n",
    "    # Initialize (or reload) the parameters using 'model_options'\n",
    "    # then build the tensorflow graph\n",
    "    logger.info(\"init_word_embedding\")\n",
    "    params = init_params(model_options, worddicts)\n",
    "    embedding = word_embedding(model_options, params)\n",
    "    is_training, cost, x, x_mask, y, n_timesteps, group_pred, summary = build_model(embedding, model_options)\n",
    "    #is_training, cost, x, x_mask, y, n_timesteps, pred, summary = build_model(embedding, model_options)\n",
    "    with tf.variable_scope('train'):\n",
    "        lr = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "        def assign_lr(session, lr_value):\n",
    "            session.run(tf.assign(lr, lr_value))\n",
    "\n",
    "        logger.info('Building optimizers...')\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(learning_rate=lr,rho=0.95)\n",
    "        logger.info('Done')\n",
    "        # print all variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            print(var.name, var.shape)\n",
    "        lossL = tf.add_n([tf.nn.l2_loss(v) for v in tvars if ('embeddings' not in v.name and 'bias' not in v.name)])#\n",
    "        lossL2=lossL * 0.0005\n",
    "        print(\"don't do L2 variables:\")\n",
    "        print([v.name for v in tvars if ('embeddings' in v.name or 'bias' in v.name)])\n",
    "        print(\"\\n do L2 variables:\")\n",
    "        print([v.name for v in tvars if ('embeddings' not in v.name and 'bias' not in v.name)])\n",
    "        cost = cost + lossL2\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), model_options['clip_c'])\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        # train_op = optimizer.minimize(cost)\n",
    "        op_loss = tf.reduce_mean(cost)\n",
    "        op_L2 = tf.reduce_mean(lossL)\n",
    "        logger.info(\"correct_pred\")\n",
    "        correct_pred = tf.equal(group_pred, y)  # make prediction\n",
    "        #correct_pred = tf.equal(tf.argmax(input=pred, axis=1), y)  # make prediction\n",
    "        logger.info(\"Done\")\n",
    "\n",
    "        temp_accuracy = tf.cast(correct_pred, tf.float32)  # change to float32\n",
    "\n",
    "    logger.info(\"init variables\")\n",
    "    init = tf.global_variables_initializer()\n",
    "    logger.info(\"Done\")\n",
    "    # saver\n",
    "    saver = tf.train.Saver(max_to_keep=15)\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    # config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "    config.gpu_options.allow_growth = True\n",
    "    #gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    #sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))  \n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "        #sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "        training_writer = tf.summary.FileWriter(\"results/MIL/logs/{}/training\".format(RUN_NAME), sess.graph)\n",
    "        validate_writer = tf.summary.FileWriter(\"results/MIL/logs/{}/validate\".format(RUN_NAME), sess.graph)\n",
    "        testing_writer = tf.summary.FileWriter(\"results/MIL/logs/{}/testing\".format(RUN_NAME), sess.graph)\n",
    "        sess.run(init)\n",
    "        history_errs = []\n",
    "        history_valid_result = []\n",
    "        history_test_result = []\n",
    "        # reload history\n",
    "        if reload_ and os.path.exists(saveto):\n",
    "            logger.info(\"Reload history error\")\n",
    "            history_errs = list(numpy.load(saveto)['history_errs'])\n",
    "\n",
    "        bad_counter = 0\n",
    "\n",
    "        if validFreq == -1:\n",
    "            validFreq = len(train[0]) / batch_size\n",
    "        if saveFreq == -1:\n",
    "            saveFreq = len(train[0]) / batch_size\n",
    "        \n",
    "        loss_plot=defaultdict(list)\n",
    "        uidx = 0\n",
    "        estop = False\n",
    "        valid_acc_record = []\n",
    "        test_acc_record = []\n",
    "        best_num = -1\n",
    "        best_epoch_num = 0\n",
    "        lr_change_list = []\n",
    "        fine_tune_flag = 1\n",
    "        wait_counter = 0\n",
    "        wait_N = model_options['wait_N']\n",
    "        learning_rate = model_options['lrate']\n",
    "        assign_lr(sess, learning_rate)\n",
    "        for eidx in range(max_epochs):\n",
    "            n_samples = 0\n",
    "            training_cost = 0\n",
    "            training_acc = 0\n",
    "            #for x, instan_gath,y, maxlen__x in train:\n",
    "            for x,y in train:\n",
    "                n_samples += len(x)\n",
    "                uidx += 1\n",
    "                keep_prob = model_options['keep_prob']\n",
    "                is_training = True\n",
    "                data_x, data_x_mask,data_y = prepare_data(x,y,model_options,maxlen=maxlen)\n",
    "                print(data_x.shape, data_x_mask.shape, data_y.shape)\n",
    "                assert data_y.shape[0] == data_x.shape[0], 'Size does not match'\n",
    "                if x is None:\n",
    "                    logger.debug('Minibatch with zero sample under length {0}'.format(maxlen))\n",
    "                    uidx -= 1\n",
    "                    continue\n",
    "                ud_start = time.time()\n",
    "                _, loss,loss_no_mean,temp_acc,l2_check = sess.run([train_op, op_loss,cost,temp_accuracy,op_L2],\n",
    "                                   feed_dict={'input/x:0': data_x, 'input/x_mask:0': data_x_mask, 'input/y:0': data_y,\n",
    "                                              'input/keep_prob:0': keep_prob, 'input/is_training:0': is_training})\n",
    "                ud = time.time() - ud_start\n",
    "                training_cost += loss_no_mean.sum()\n",
    "                training_acc += temp_acc.sum()\n",
    "                loss_plot['training'].append(loss)\n",
    "                '''train_summary = sess.run(summary, feed_dict={'input/x:0': data_x, 'input/x_mask:0': data_x_mask,\n",
    "                                                              'input/y:0': data_y,'input/keep_prob:0':keep_prob,'input/is_training:0':is_training})\n",
    "                training_writer.add_summary(train_summary, eidx)'''\n",
    "                if numpy.mod(uidx, dispFreq) == 0:\n",
    "                    logger.debug('Epoch {0} Update {1} Cost {2} L2 {3} TIME {4}'.format(eidx, uidx, loss,l2_check,ud))\n",
    "\n",
    "                # validate model on validation set and early stop if necessary\n",
    "                if numpy.mod(uidx, validFreq) == 0:\n",
    "                    is_training = False\n",
    "                    \n",
    "                    #pred-->group_pred\n",
    "                    valid_acc, valid_loss,valid_final_result = predict_pro_acc(sess, cost, prepare_data, model_options, valid, maxlen,\n",
    "                                                            correct_pred, group_pred, summary, eidx, is_training, train_op,loss_plot,\n",
    "                                                            validate_writer,validate=True)\n",
    "                    test_acc, test_loss,test_final_result = predict_pro_acc(sess, cost, prepare_data, model_options, test, maxlen,\n",
    "                                                          correct_pred, group_pred, summary, eidx, is_training, train_op,loss_plot,\n",
    "                                                          testing_writer)\n",
    "                    # valid_err = 1.0 - valid_acc\n",
    "                    valid_err = valid_loss\n",
    "                    history_errs.append(valid_err)\n",
    "                    history_valid_result.append(valid_final_result)\n",
    "                    history_test_result.append(test_final_result)\n",
    "                    loss_plot['validate_ep'].append(valid_loss)\n",
    "                    loss_plot['testing_ep'].append(test_loss)\n",
    "                    logger.debug('Epoch  {0}'.format(eidx))\n",
    "                    logger.debug('Valid cost  {0}'.format(valid_loss))\n",
    "                    logger.debug('Valid accuracy  {0}'.format(valid_acc))\n",
    "                    logger.debug('Test cost  {0}'.format(test_loss))\n",
    "                    logger.debug('Test accuracy  {0}'.format(test_acc))\n",
    "                    logger.debug('learning_rate:  {0}'.format(learning_rate))\n",
    "\n",
    "                    valid_acc_record.append(valid_acc)\n",
    "                    test_acc_record.append(test_acc)\n",
    "                    if uidx == 0 or valid_err <= numpy.array(history_errs).min():\n",
    "                        best_num = best_num + 1\n",
    "                        best_epoch_num = eidx\n",
    "                        wait_counter = 0\n",
    "                        logger.info(\"Saving...\")\n",
    "                        saver.save(sess, _s(_s(_s(save_model, \"epoch\"), str(best_num)), \"model.ckpt\"))\n",
    "                        logger.info(_s(_s(_s(save_model, \"epoch\"), str(best_num)), \"model.ckpt\"))\n",
    "                        numpy.savez(saveto, history_errs=history_errs, **params)\n",
    "                        pkl.dump(model_options, open('{}.pkl'.format(saveto), 'wb'))\n",
    "                        logger.info(\"Done\")\n",
    "\n",
    "                    if valid_err > numpy.array(history_errs).min():\n",
    "                        wait_counter += 1\n",
    "                    # wait_counter +=1 if valid_err>numpy.array(history_errs).min() else 0\n",
    "                    if wait_counter >= wait_N:\n",
    "                        logger.info(\"wait_counter max, need to half the lr\")\n",
    "                        # print 'wait_counter max, need to half the lr'\n",
    "                        bad_counter += 1\n",
    "                        wait_counter = 0\n",
    "                        logger.debug('bad_counter:  {0}'.format(bad_counter))\n",
    "                        # TODO change the learining rate\n",
    "                        #learning_rate = learning_rate * 0.9\n",
    "                        # learning_rate = learning_rate\n",
    "                        #assign_lr(sess, learning_rate)\n",
    "                        lr_change_list.append(eidx)\n",
    "                        logger.debug('lrate change to:   {0}'.format(learning_rate))\n",
    "                        # print 'lrate change to: ' + str(lrate)\n",
    "\n",
    "                    if bad_counter > patience and fine_tune_flag == 0:\n",
    "                        logger.debug('ATTENTION! INTO FINE TUNING STAGE !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                        optimizer = tf.train.MomentumOptimizer(learning_rate=0.000001, momentum=0.6)\n",
    "                        fine_tune_flag = 1\n",
    "                        bad_counter = 0\n",
    "                    if bad_counter > patience and fine_tune_flag == 1:\n",
    "                        logger.info(\"Early Stop!\")\n",
    "                        estop = True\n",
    "                        break\n",
    "\n",
    "                    if numpy.isnan(valid_err):\n",
    "                        pdb.set_trace()\n",
    "\n",
    "                        # finish after this many updates\n",
    "                if uidx >= finish_after:\n",
    "                    logger.debug('Finishing after iterations!  {0}'.format(uidx))\n",
    "                    # print 'Finishing after %d iterations!' % uidx\n",
    "                    estop = True\n",
    "                    break\n",
    "            logger.debug('Seen samples:  {0}'.format(n_samples))\n",
    "            logger.debug('Training accuracy:  {0}'.format(1.0 * training_acc/n_samples))\n",
    "            loss_plot['training_ep'].append(training_cost/n_samples)\n",
    "            # print 'Seen %d samples' % n_samples\n",
    "            logger.debug('results/MIL/Saved loss_plot pickle')\n",
    "            with open(\"results/MIL/important_plot.pickle\",'wb') as handle:\n",
    "                pkl.dump(loss_plot, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "            if estop:\n",
    "                break\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, _s(_s(_s(save_model, \"epoch\"), str(best_num)), \"model.ckpt\"))\n",
    "        keep_prob = 1\n",
    "        is_training = False\n",
    "        logger.info('=' * 80)\n",
    "        logger.info('Final Result')\n",
    "        logger.info('=' * 80)\n",
    "        logger.debug('best epoch   {0}'.format(best_epoch_num))\n",
    "\n",
    "        #pred-->group_pred\n",
    "        valid_acc, valid_cost,valid_final_result = predict_pro_acc(sess, cost, prepare_data, model_options, valid,\n",
    "                                                maxlen, correct_pred, group_pred, summary, eidx,train_op, is_training, None)\n",
    "        logger.debug('Valid cost   {0}'.format(valid_cost))\n",
    "        logger.debug('Valid accuracy   {0}'.format(valid_acc))\n",
    "\n",
    "        # print 'Valid cost', valid_cost\n",
    "        # print 'Valid accuracy', valid_acc\n",
    "\n",
    "        test_acc, test_cost,test_final_result = predict_pro_acc(sess, cost, prepare_data, model_options, test,\n",
    "                                              maxlen, correct_pred, group_pred, summary, eidx,train_op, is_training, None)\n",
    "        logger.debug('Test cost   {0}'.format(test_cost))\n",
    "        logger.debug('Test accuracy   {0}'.format(test_acc))\n",
    "\n",
    "        # print 'best epoch ', best_epoch_num\n",
    "        train_acc, train_cost,_ = predict_pro_acc(sess, cost, prepare_data, model_options, train_valid,\n",
    "                                                maxlen, correct_pred, group_pred, summary, eidx,train_op, is_training, None)\n",
    "        logger.debug('Train cost   {0}'.format(train_cost))\n",
    "        logger.debug('Train accuracy   {0}'.format(train_acc))\n",
    "        valid_m=numpy.array(history_valid_result)\n",
    "        test_m=numpy.array(history_test_result)\n",
    "        valid_final_result = (numpy.array([valid_final_result])==False)\n",
    "        test_final_result = (numpy.array([test_final_result])==False)\n",
    "        #print(numpy.all(valid_m, axis = 0))\n",
    "        #print(numpy.all(test_m, axis=0))\n",
    "        print('validation: all prediction through every epoch that are the same:',numpy.where(numpy.all(valid_m, axis = 0)))\n",
    "        print('testing: all prediction through every epoch that are the same:',numpy.where(numpy.all(test_m, axis=0)))\n",
    "        print('validation: final prediction that is False:',numpy.where(valid_final_result))\n",
    "        print('testing: final prediction that is False:',numpy.where(test_final_result))\n",
    "        if os.path.exists('results/MIL/history_predict.npz'):\n",
    "            logger.info(\"Load and save to history_predict.npz\")\n",
    "            valid_history = numpy.load('results/MIL/history_predict.npz')['valid_final_result']\n",
    "            test_history = numpy.load('results/MIL/history_predict.npz')['test_final_result']\n",
    "            vv=numpy.concatenate((valid_history,valid_final_result),axis=0)\n",
    "            tt=numpy.concatenate((test_history,valid_final_result),axis=0)\n",
    "            print('Concate shape valid:',vv.shape)\n",
    "            print('Print all validate history outputs that return False',numpy.where(numpy.all(vv,axis=0)))\n",
    "            print('Concate shape test:',tt.shape)\n",
    "            print('Print all test history outputs that return False',numpy.where(numpy.all(tt,axis=0)))\n",
    "            numpy.savez('results/MIL/history_predict.npz',valid_final_result=vv,test_final_result=tt,**params)\n",
    "        else:\n",
    "            numpy.savez('results/MIL/history_predict.npz',valid_final_result=valid_final_result,test_final_result=test_final_result,**params)\n",
    "        # print 'Train cost', train_cost\n",
    "        # print 'Train accuracy', train_acc\n",
    "\n",
    "        # print 'Test cost   ', test_cost\n",
    "        # print 'Test accuracy   ', test_acc\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
